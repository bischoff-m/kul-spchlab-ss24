{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1-Sg0pH041mX"
   },
   "source": [
    "## Classification of Speech Frames\n",
    "PART IV: RECOGNIZE A PHONEME FROM FILTERBANK FEATURES WITH A DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nEBOGxM441mY"
   },
   "source": [
    "### 1. Setting up your Python Environment\n",
    "\n",
    "1. Import Python's Machine Learning Stack and other utilities\n",
    "\n",
    "2. Quite a few helper routines are defined for learning our Neural Network in Pytorch in the pyspch.nn module.\n",
    "\n",
    "#### ISSUES: \n",
    "- 30/03/2022:   problem opening/importing current version on PC/windows 'code editor out of sync'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import io, os, sys\n",
    "import logging\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from scipy.fftpack import dct\n",
    "#from IPython.display import display\n",
    "\n",
    "# reproducibility \n",
    "torch.manual_seed(0) \n",
    "np.random.seed(0)\n",
    "\n",
    "# pyspch\n",
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "  ! pip install git+https://github.com/compi1234/pyspch.git@v0.6\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "\n",
    "# pyspch\n",
    "import pyspch\n",
    "import pyspch.nn\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Auxiliary functions \n",
    "\n",
    "import requests\n",
    "import importlib\n",
    "import scipy.io as sio\n",
    "import urllib.request\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "# download from url and write to file\n",
    "def write_from_url(url, filename):\n",
    "    r = requests.get(url)\n",
    "    with open(filename, 'wb') as f:\n",
    "        f.write(r.content)\n",
    "\n",
    "# dictionairy\n",
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "    \n",
    "def dict_from_module(module):\n",
    "    context = {}\n",
    "    for setting in dir(module):\n",
    "        # you can write your filter here\n",
    "        if not setting.startswith('_'):\n",
    "            context[setting] = getattr(module, setting)\n",
    "\n",
    "    return context\n",
    "\n",
    "# import setup file (.py) as module (dotdict)\n",
    "def read_setup(filename, new_read_path=None, old_read_path='/users/spraak/spchlab/public_html/data/timit/'):\n",
    "    \n",
    "    # load module\n",
    "    spec = importlib.util.spec_from_file_location(os.path.basename(filename), filename)\n",
    "    setup = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(setup)\n",
    "    \n",
    "    # convert to dict \n",
    "    setup = dict_from_module(setup)\n",
    "    setup = dotdict(setup)\n",
    "\n",
    "    # replace esat_path with root_url\n",
    "    if new_read_path is not None:\n",
    "        for k, v in setup.items():\n",
    "            if type(v) == str:\n",
    "                setup[k] = v.replace(old_read_path, new_read_path)\n",
    "            \n",
    "    return setup\n",
    "\n",
    "# loads all data in a matlab file at given url to the contents structure\n",
    "# this is working for MATLAB 7.0 files and older ; not hdf5 MATLAB 7.3 or more recent\n",
    "def load_matlab_from_url(url):\n",
    "    url_response = urllib.request.urlopen(url)\n",
    "    matio = io.BytesIO(url_response.read())\n",
    "    contents = sio.loadmat(matio,squeeze_me=True)\n",
    "    return(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device\n",
    "use_cuda_if_available = True\n",
    "if use_cuda_if_available:\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r3ezItDBC8y9"
   },
   "source": [
    "### Baseline DNN model\n",
    "\n",
    "We use a simple Neural Network (NN) to classify the frames, namely a fully connected feed-forward network, with a sigmoid activation function following each hidden layer. The architecture is defined by the input, output and hidden layer sizes. \n",
    "\n",
    "The input feature is a window of 11 frames (black) around the target frame (red) with a stride of 2, giving window that spans 210 ms. \n",
    "The stride is distance (in time steps) between the selected frames. \n",
    "For each target frames, the selected frames are concatenated into a vector.\n",
    "Alternatively, one could preserve the 2D structure of the input (time x frequency) provided the neural architecture allows it (e.g. using 2D convolutional layers). \n",
    "\n",
    "The phone labels are one-hot encoded, we thus have a neuron for each phone label in the output layer of the network. By applying the softmax function at the end of the network, we obtain phone probabilities as outputs. The predicted label is the one with the highest probabilty. \n",
    "\n",
    "Note that training a model from scratch on TIMIT is (relatively) computationally expensive. That is why we load a pretrained model, matching the architecture described above.\n",
    "\n",
    "<img src=\"http://homes.esat.kuleuven.be/~spchlab/data_old/timit/dnn_setup_3.PNG\" width=\"750\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup for baseline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root url\n",
    "root_url = 'https://homes.esat.kuleuven.be/~spchlab/data/timit/'\n",
    "\n",
    "# baseline path\n",
    "baseline_path = 'models/default/mfcc13dd2v/N5s2/'\n",
    "# baseline_path = 'models/dummy/'\n",
    "os.makedirs(baseline_path, exist_ok=True, mode=0o777)\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "#model_fobj = pyspch.read_fobj(root_url + model_path + 'model.pt')\n",
    "#checkpoint = pyspch.nn.read_checkpoint(model_fobj, device)\n",
    "    \n",
    "# setup file\n",
    "write_from_url(root_url + baseline_path + 'setup.py', baseline_path + 'setup.py')\n",
    "write_from_url(root_url + baseline_path + 'model.pt', baseline_path + 'model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read setup \n",
    "setup = read_setup(baseline_path + 'setup.py', root_url)\n",
    "\n",
    "# read checkpoint (model)\n",
    "checkpoint = pyspch.nn.read_checkpoint(baseline_path + 'model.pt', device)\n",
    "model_setup, lab2idx, model, criterion, optimizer, scheduler = checkpoint\n",
    "model.to(device)\n",
    "\n",
    "# update setup - allows continued training\n",
    "setup.update(model_setup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ndcw8aPpVGaU"
   },
   "source": [
    "#### Data loading\n",
    "\n",
    "Apart from using a window of frames as input feature for each target frame, we can also normalize and add temporal derrivatives to input feature. \n",
    "Here we normalize the variance per channel (and per utterance) and add a first order temporal derrivative (called delta).  \n",
    "\n",
    "Next we prepare our test data set according to our feature extraction setup.\n",
    "Note we don't need the training or validation data since we're using a pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Deltas': 'delta_delta2', 'Norm': 'var'}\n"
     ]
    }
   ],
   "source": [
    "# feature arguments\n",
    "# feature_path = root_url + 'features/mel24/'\n",
    "read_feature_args = pyspch.read_json(setup.feature_path + 'feature_args.json')\n",
    "modify_feature_args = pyspch.dct_diff(read_feature_args, setup.feature_args)\n",
    "print(modify_feature_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'aa': 0, 'ae': 1, 'ah': 2, 'ao': 3, 'aw': 4, 'er': 5, 'ay': 6, 'b': 7, 'ch': 8, 'd': 9, 'dh': 10, 'eh': 11, 'm': 12, 'ng': 13, 'ey': 14, 'f': 15, 'g': 16, 'hh': 17, 'ih': 18, 'iy': 19, 'jh': 20, 'k': 21, 'l': 22, 'n': 23, 'ow': 24, 'oy': 25, 'p': 26, 'r': 27, 's': 28, 'sh': 29, 't': 30, 'th': 31, 'uh': 32, 'uw': 33, 'v': 34, 'w': 35, 'y': 36, 'z': 37, 'zh': 38, 'sil': 39, 'cl': 40}\n"
     ]
    }
   ],
   "source": [
    "# label mapping \n",
    "labels = pyspch.timit.get_timit_alphabet(setup.labset_out)\n",
    "lab2lab = pyspch.timit.get_timit_mapping(setup.labset_in, setup.labset_out) \n",
    "print(lab2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test set (evaluation)\n",
    "\n",
    "# corpus\n",
    "test_corpus = pyspch.read_txt(setup.test_corpus_file)\n",
    "\n",
    "# read features \n",
    "test_df = pd.read_pickle(setup.test_pickle_file)\n",
    "test_data = pyspch.nn.DataFrame_to_SpchData(test_df, delete_df=True) \n",
    "\n",
    "# modify features in corpus\n",
    "test_data = test_data.subset(test_corpus) \n",
    "test_data.modify_features(modify_feature_args) \n",
    "\n",
    "# SpchDataset\n",
    "test_ds = pyspch.nn.SpchDataset(test_data.corpus, test_data.features, test_data.labels)\n",
    "test_ds.map_target(lab2lab) # timit61 -> timit41\n",
    "test_ds.encode_target(lab2idx) # one-hot encoding\n",
    "test_ds.to_tensor()\n",
    "\n",
    "# Sampler (splicing)\n",
    "test_lengths = test_data.get_length('features')\n",
    "test_ds.set_sampler(test_lengths, setup.sampler_args)\n",
    "test_ds.to_device(device)\n",
    "\n",
    "# DataLoader\n",
    "test_dl = torch.utils.data.DataLoader(test_ds, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test set: PER and confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = pyspch.nn.evaluate_cm(model, test_dl) \n",
    "\n",
    "# Phone Error Rate (PER) + PER per phone class\n",
    "per, per_pc = pyspch.nn.cm2per(cm)\n",
    "print(\"PER %.2f \" % (per))\n",
    "if not None in per_pc:\n",
    "    print(\"PER per phone class %s\" % (np.round(per_pc, 4)))\n",
    "\n",
    "# Cross-entropy loss\n",
    "loss = pyspch.nn.evaluate(model, test_dl, criterion)\n",
    "print(\"CE Loss %.2f\" % loss)\n",
    "\n",
    "# plot\n",
    "pyspch.plot_confusion_matrix(cm, labels, annot=False, figsize=(20,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Single utterance: phone probabilties\n",
    "\n",
    "The posterior probabilties are now visualized for one entire utterance below. Because the dataset (after feature extraction) is quite big, we avoid loading unnecessary data. Here we only do the feature extraction for a single utterance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select file to visualize\n",
    "example = 'test/dr1/faks0/si2203'\n",
    "\n",
    "# transcription + word segmentation \n",
    "example_txt = pyspch.read_txt(setup.label_path + example + \".txt\")\n",
    "example_wrd = pyspch.read_dataframe(setup.label_path + example + \".wrd\", sep=\" \", names=['t0','t1','wrd'])\n",
    "\n",
    "# labels (phone segmentation)\n",
    "lab_shift = read_feature_args['f_shift'] * read_feature_args['sample_rate']\n",
    "example_phn = pyspch.timit.read_seg_file(setup.label_path + example + \".phn\", fmt=\"float32\", dt=1/16000)\n",
    "example_lab = pyspch.seg2lbls(example_phn) #length !!!\n",
    "example_lab = [lab2lab[lbl] for lbl in example_lab]\n",
    "example_idx = [lab2idx[lbl] for lbl in example_lab] # target\n",
    "\n",
    "# audio\n",
    "example_audio, _ = pyspch.audio.load(root_url + 'audio/' + example + \".wav\")\n",
    "\n",
    "# feature + modification + splicing\n",
    "example_feature = np.load(pyspch.read_fobj(setup.feature_path + example + '.npy'))\n",
    "example_mod = pyspch.sp.feature_extraction(spg=example_feature, **modify_feature_args)\n",
    "example_spliced = pyspch.sp.splice_frames(example_mod, setup.sampler_args['N'], setup.sampler_args['stride']) # input\n",
    "\n",
    "# tensor\n",
    "example_X = torch.tensor(example_spliced).T.float().to(device)\n",
    "example_y = torch.tensor(example_idx).long().to(device)\n",
    "\n",
    "# prediction \n",
    "example_yp = model(example_X) # log probs\n",
    "example_yp = torch.nn.Softmax(dim=1)(example_yp) # probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<SpchFig size 864x432 with 1 Axes>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualization\n",
    "pyspch.display.PlotSpg(spgdata=example_feature, segspg=example_phn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l_gUZsrR3x3b"
   },
   "outputs": [],
   "source": [
    "# plot only subset of phones (for readability)\n",
    "long_vowels = ['aw', 'ay', 'er', 'ey', 'iy', 'ow', 'oy', 'uw']\n",
    "short_vowels = ['aa', 'ao', 'ae', 'ah', 'eh', 'ih', 'uh']\n",
    "\n",
    "# plot input\n",
    "if False:\n",
    "  utt_fig, utt_ax = plt.subplots(figsize=(15, 10))\n",
    "  utt_ax.matshow(example_X.cpu().detach().numpy().T)\n",
    "\n",
    "# plot output - posterior probabilities\n",
    "plot_df = pd.DataFrame(example_yp.cpu().detach().numpy(), columns=labels)\n",
    "cols = labels\n",
    "cols = ['sil'] + short_vowels # choose phones to plot (subset) \n",
    "# Note: sil = silence and vcl = 'voiced closure' ~ silence\n",
    "plot_df[cols].plot(title=\"Predicted phone probabilties\", figsize=(15, 3), xlim=[0, example_yp.shape[0]])\n",
    "\n",
    "# plot output - posterior probabilities\n",
    "cols = ['sil'] + long_vowels # choose phones to plot (subset) \n",
    "plot_df[cols].plot(title=\"Predicted phone probabilties\", figsize=(15, 3), xlim=[0, example_yp.shape[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ayLeimW7KYk"
   },
   "source": [
    "#### Questions\n",
    "\n",
    "**Phone mapping**\n",
    "\n",
    "Q1. What is the effect of reducing the number of phone labels (e.g. from 61 to 48) for training the model? \n",
    "\n",
    "**Feature extraction**\n",
    "\n",
    "Q2. Can we use our pretrained model with a different feature extraction setup?\n",
    "\n",
    "Q3. What happens to the input feature on the phone boundaries?\n",
    "\n",
    "Q4. What happens to the input feature on the utterance boundaries?\n",
    "\n",
    "**Evaluation**\n",
    "\n",
    "Q5. What is the effect of reducing the number of phone labels from 48 to 39 for evaluating the model? \n",
    "\n",
    "<!-- Q6. Given the posterior probabilities for the utterance \"The reasons for this dive seemed foolish now\", is there a corespondence between the short vowels that are hard to recognize for the model and your own perception? -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OWAzcb7HXsGb"
   },
   "source": [
    "### Effect of input features\n",
    "\n",
    "Using the same model setup as above, we will now vary the feature extraction setup. Four new setups are considered. Note that we are still loading pretrained models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write multiple pretrained models \n",
    "\n",
    "# models/architecture/feature/sampler/\n",
    "model_paths = ['models/default/mfcc13dd2mv/N5s2/', # 11 frames, stride 2 (210 ms), delta_delta2 (+ 40 ms) \n",
    "               'models/default/mel24dd2mv/N5s2/', # 11 frames, stride 2 (210 ms), delta_delta2 (+ 40 ms) \n",
    "               'models/default/mel80mv/N5s2/', # 11 frames, stride 2 (210 ms)\n",
    "               'models/default/mfcc13dd2mv/N0s1/', # 1 frames, stride 2 (10 ms), delta_delta2 (+ 40 ms) \n",
    "               'models/default/mel24dd2mv/N0s1/', # 1 frames, stride 2 (10 ms), delta_delta2 (+ 40 ms) \n",
    "               'models/default/mel80mv/N0s1/', # 1 frames, stride 2 (10 ms)\n",
    "               ]\n",
    "\n",
    "for model_path in model_paths:\n",
    "    os.makedirs(model_path, exist_ok=True, mode=0o777)\n",
    "    write_from_url(root_url + model_path + 'setup.py', model_path + 'setup.py')\n",
    "    write_from_url(root_url + model_path + 'model.pt', model_path + 'model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate and visualize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shared (base) feature = mel24\n",
    "test_corpus = pyspch.read_txt(setup.test_corpus_file) # corpus\n",
    "test_df = pd.read_pickle(setup.test_pickle_file) # read all features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 924
    },
    "id": "tNHqOxbldQC3",
    "outputId": "b8f0af33-008c-44f5-98d2-07b448e4cf6d"
   },
   "outputs": [],
   "source": [
    "for model_path in model_paths: \n",
    "    \n",
    "    print(model_path)\n",
    "    \n",
    "    ### Model\n",
    "    \n",
    "    # setup and model\n",
    "    setup = read_setup(model_path + 'setup.py', root_url)\n",
    "    checkpoint = pyspch.nn.read_checkpoint(model_path + 'model.pt', device)\n",
    "    model_setup, lab2idx, model, criterion, optimizer, scheduler = checkpoint\n",
    "    model.to(device)\n",
    "    \n",
    "    # feature arguments\n",
    "    read_feature_args = pyspch.read_json(setup.feature_path + 'feature_args.json')\n",
    "    modify_feature_args = pyspch.dct_diff(read_feature_args, setup.feature_args)\n",
    "    \n",
    "    # label mapping \n",
    "    labels = pyspch.timit.get_timit_alphabet(setup.labset_out)\n",
    "    lab2lab = pyspch.timit.get_timit_mapping(setup.labset_in, setup.labset_out) \n",
    "    \n",
    "    ### Single utterance: phone probabilties\n",
    "    \n",
    "    # utterance\n",
    "    example = 'test/dr1/faks0/si2203'\n",
    "    \n",
    "    # feature + modification + splicing\n",
    "    example_feature = np.load(pyspch.read_fobj(setup.feature_path + example + '.npy'))\n",
    "    example_mod = pyspch.sp.feature_extraction(spg=example_feature, **modify_feature_args)\n",
    "    example_spliced = pyspch.sp.splice_frames(example_mod, setup.sampler_args['N'], setup.sampler_args['stride']) # input\n",
    "\n",
    "    # labels (phone segmentation)\n",
    "    lab_shift = read_feature_args['f_shift'] * read_feature_args['sample_rate']\n",
    "    example_phn = pyspch.timit.read_seg_file(setup.label_path + example + \".phn\", fmt=\"float32\")\n",
    "    example_lab = pyspch.seg2lbls(example_phn, lab_shift)\n",
    "    example_lab = [lab2lab[lbl] for lbl in example_lab]\n",
    "    example_idx = [lab2idx[lbl] for lbl in example_lab] # target\n",
    "    \n",
    "    # tensor\n",
    "    example_X = torch.tensor(example_spliced).T.float().to(device)\n",
    "    example_y = torch.tensor(example_idx).long().to(device)\n",
    "\n",
    "    # prediction \n",
    "    example_yp = model(example_X) # log probs\n",
    "    example_yp = torch.nn.Softmax(dim=1)(example_yp) # probs\n",
    "    \n",
    "    # plot output - posterior probabilities\n",
    "    plot_df = pd.DataFrame(example_yp.cpu().detach().numpy())\n",
    "    plot_df.columns = labels\n",
    "    cols = plot_df.columns # choose phones to plot\n",
    "    cols = ['sil'] + short_vowels # + long_vowels # \n",
    "    plot_df[cols].plot(title=\"Predicted phone probabilties\", figsize=(15, 3))\n",
    "\n",
    "    if False:\n",
    "      \n",
    "        ## Test set: PER and confusion matrix\n",
    "        \n",
    "        # modify features\n",
    "        test_data = pyspch.nn.DataFrame_to_SpchData(test_df, delete_df=False)  \n",
    "        test_data = test_data.subset(test_corpus) # subset by corpus\n",
    "        test_data.modify_features(modify_feature_args) # modify features\n",
    "\n",
    "        # SpchDataset\n",
    "        test_ds = pyspch.nn.SpchDataset(test_data.corpus, test_data.features, test_data.labels)\n",
    "        test_ds.map_target(lab2lab) # timit61 -> timit41\n",
    "        test_ds.encode_target(lab2idx) # one-hot encoding\n",
    "        test_ds.to_tensor()\n",
    "\n",
    "        # Sampler (splicing)\n",
    "        test_lengths = test_data.get_length('features')\n",
    "        test_ds.set_sampler(test_lengths, setup.sampler_args)\n",
    "\n",
    "        # DataLoader\n",
    "        test_dl = torch.utils.data.DataLoader(test_ds, batch_size=256)\n",
    "        \n",
    "        # PER and confusion matrix\n",
    "        loss = pyspch.nn.evaluate(model, test_dl, criterion)\n",
    "        cm = pyspch.nn.evaluate_cm(model, test_dl) \n",
    "        per, per_pc = pyspch.nn.cm2per(cm)\n",
    "        print(\"CEL %.2f\" % loss)\n",
    "        print(\"PER %.2f \" % (per))\n",
    "        pyspch.plot_confusion_matrix(cm, labels, annot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "26x3pC1WaaMk"
   },
   "source": [
    "#### Questions\n",
    "\n",
    "**Example utterance**\n",
    "\n",
    "Q1. Do you notice a difference in the posterior probabilities for different input features?\n",
    "\n",
    "Q2. Evaluate the models on the test set (run this only if you have spare time). Do you notice a difference in the PER for different input features? Which feature gives the best performance, and why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3AbGPiN70Kt0"
   },
   "source": [
    "### DNN vs. GMM on single frame \n",
    "\n",
    "Last exercise session on phoneme classification, we compared the performance of DNNs and GMMs. Near the end we remarked the toy problem was too small for meaningful conclusions. Here we compare the two models on a subset of TIMIT, namely the short vowels. \n",
    "\n",
    "If you want to run small experiments yourself, you can select a subset with only a couple of phonemes (reducing the size of the dataset and the diversity in labels). Here we use the short vowels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nhOYltapZNje"
   },
   "outputs": [],
   "source": [
    "# subset of phone labels\n",
    "short_vowels = ['aa', 'ao', 'ae', 'ah', 'eh', 'ih', 'uh']\n",
    "long_vowels = ['aw', 'ay', 'er', 'ey', 'iy', 'ow', 'oy', 'uw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline path\n",
    "single_frame_path = 'models/default/mfcc13dd2mv/N0s1/'\n",
    "os.makedirs(single_frame_path, exist_ok=True, mode=0o777)\n",
    "\n",
    "# setup file\n",
    "write_from_url(root_url + single_frame_path + 'setup.py', single_frame_path + 'setup.py')\n",
    "write_from_url(root_url + single_frame_path + 'model.pt', single_frame_path + 'model.pt')\n",
    "setup = read_setup(single_frame_path + 'setup.py', root_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare train set \n",
    "\n",
    "# corpus\n",
    "train_corpus = pyspch.read_txt(setup.train_corpus_file)\n",
    "\n",
    "# read features \n",
    "train_df = pd.read_pickle(setup.train_pickle_file)\n",
    "train_data = pyspch.nn.DataFrame_to_SpchData(train_df, delete_df=True) \n",
    "\n",
    "# modify features in corpus\n",
    "train_data = train_data.subset(train_corpus) \n",
    "train_data.modify_features(modify_feature_args) \n",
    "\n",
    "# SpchDataset\n",
    "train_ds = pyspch.nn.SpchDataset(train_data.corpus, train_data.features, train_data.labels)\n",
    "train_ds.map_target(lab2lab) # timit61 -> timit41\n",
    "train_ds.encode_target(lab2idx) # one-hot encoding\n",
    "train_ds.to_tensor()\n",
    "\n",
    "# Sampler (splicing)\n",
    "train_lengths = train_data.get_length('features')\n",
    "train_ds.set_sampler(train_lengths, setup.sampler_args)\n",
    "train_ds.to_device(device)\n",
    "\n",
    "# DataLoader\n",
    "batch_size, shuffle, num_workers = setup.training_args['batch_size'], setup.training_args['shuffle'], setup.training_args['num_workers']\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test set (evaluation)\n",
    "\n",
    "# corpus\n",
    "test_corpus = pyspch.read_txt(setup.test_corpus_file)\n",
    "\n",
    "# read features \n",
    "test_df = pd.read_pickle(setup.test_pickle_file)\n",
    "test_data = pyspch.nn.DataFrame_to_SpchData(test_df, delete_df=True) \n",
    "\n",
    "# modify features in corpus\n",
    "test_data = test_data.subset(test_corpus) \n",
    "test_data.modify_features(modify_feature_args) \n",
    "\n",
    "# SpchDataset\n",
    "test_ds = pyspch.nn.SpchDataset(test_data.corpus, test_data.features, test_data.labels)\n",
    "test_ds.map_target(lab2lab) # timit61 -> timit41\n",
    "test_ds.encode_target(lab2idx) # one-hot encoding\n",
    "test_ds.to_tensor()\n",
    "\n",
    "# Sampler (splicing)\n",
    "test_lengths = test_data.get_length('features')\n",
    "test_ds.set_sampler(test_lengths, setup.sampler_args)\n",
    "test_ds.to_device(device)\n",
    "\n",
    "# DataLoader\n",
    "test_dl = torch.utils.data.DataLoader(test_ds, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "checkpoint = pyspch.nn.read_checkpoint(single_frame_path + 'model.pt', device)\n",
    "model_setup, lab2idx, model, criterion, optimizer, scheduler = checkpoint\n",
    "model.to(device)\n",
    "\n",
    "# feature arguments\n",
    "read_feature_args = pyspch.read_json(setup.feature_path + 'feature_args.json')\n",
    "modify_feature_args = pyspch.dct_diff(read_feature_args, setup.feature_args)\n",
    "\n",
    "# label mapping \n",
    "labels = pyspch.timit.get_timit_alphabet(setup.labset_out)\n",
    "lab2lab = pyspch.timit.get_timit_mapping(setup.labset_in, setup.labset_out) \n",
    "\n",
    "### Single utterance: phone probabilties\n",
    "\n",
    "# utterance\n",
    "example = 'test/dr1/faks0/si2203'\n",
    "\n",
    "# feature + modification + splicing\n",
    "example_feature = np.load(pyspch.read_fobj(setup.feature_path + example + '.npy'))\n",
    "example_mod = pyspch.sp.feature_extraction(spg=example_feature, **modify_feature_args)\n",
    "example_spliced = pyspch.sp.splice_frames(example_mod, setup.sampler_args['N'], setup.sampler_args['stride']) # input\n",
    "\n",
    "# labels (phone segmentation)\n",
    "lab_shift = read_feature_args['f_shift'] * read_feature_args['sample_rate']\n",
    "example_phn = pyspch.timit.read_seg_file(setup.label_path + example + \".phn\", fmt=\"float32\")\n",
    "example_lab = pyspch.seg2lbls(example_phn, lab_shift)\n",
    "example_lab = [lab2lab[lbl] for lbl in example_lab]\n",
    "example_idx = [lab2idx[lbl] for lbl in example_lab] # target\n",
    "\n",
    "# tensor\n",
    "example_X = torch.tensor(example_spliced).T.float().to(device)\n",
    "example_y = torch.tensor(example_idx).long().to(device)\n",
    "\n",
    "# prediction \n",
    "example_yp = model(example_X) # log probs\n",
    "example_yp = torch.nn.Softmax(dim=1)(example_yp) # probs\n",
    "\n",
    "# plot output - posterior probabilities\n",
    "plot_df = pd.DataFrame(example_yp.cpu().detach().numpy())\n",
    "plot_df.columns = labels\n",
    "cols = plot_df.columns # choose phones to plot\n",
    "cols = ['sil'] + short_vowels # + long_vowels # \n",
    "plot_df[cols].plot(title=\"Predicted phone probabilties\", figsize=(15, 3))\n",
    "\n",
    "if False:\n",
    "    \n",
    "    ## Test set: PER and confusion matrix\n",
    "        \n",
    "    # PER and confusion matrix\n",
    "    loss = pyspch.nn.evaluate(model, test_dl, criterion)\n",
    "    cm = pyspch.nn.evaluate_cm(model, test_dl) \n",
    "    per, per_pc = pyspch.nn.cm2per(cm)\n",
    "    print(\"CEL %.2f\" % loss)\n",
    "    print(\"PER %.2f \" % (per))\n",
    "    pyspch.plot_confusion_matrix(cm, labels, annot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GMM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare GMM data\n",
    "\n",
    "# input features\n",
    "gmm_train_X = train_ds.input.cpu().numpy().astype('float32') \n",
    "gmm_test_X = test_ds.input.cpu().numpy().astype('float32') \n",
    "\n",
    "# labels\n",
    "train_y = train_ds.target.cpu().numpy().astype('int64') \n",
    "test_y = test_ds.target.cpu().numpy().astype('int64')\n",
    "\n",
    "# map to strings for plotting\n",
    "idx2lab = {v: k for k, v in lab2idx.items()}\n",
    "train_y = np.array([idx2lab[i] for i in train_y])\n",
    "test_y = np.array([idx2lab[i] for i in test_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title GMM functions\n",
    "\n",
    "from pyspch.GaussianMixtureClf import GaussianMixtureClf\n",
    "from sklearn import metrics as skmetrics \n",
    "\n",
    "# =============================================================================\n",
    "# GMM\n",
    "# =============================================================================\n",
    "def train_GMM(X_train, y_train,  classes, \n",
    "                n_components=1, max_iter=20, tol=1.e-3,\n",
    "                print_result=True):\n",
    "    clf_GMM = GaussianMixtureClf(n_components, classes, max_iter, tol)\n",
    "    clf_GMM.fit(X_train,y_train)\n",
    "    if(print_result):\n",
    "        y_pred = clf_GMM.predict(X_train)\n",
    "        acc_train = 100.0*skmetrics.accuracy_score(y_train, y_pred)\n",
    "        lls, bics = llscore(clf_GMM,X_train,y_train)\n",
    "        print('Training Set:  Accuracy = %.2f%%     LL = %.2f    BIC = %.2f ' % (acc_train,lls,bics) )\n",
    "    return(clf_GMM)\n",
    "    \n",
    "def test_GMM(X_test, y_test, clf_GMM , priors=None,\n",
    "                   print_result=True, figsize=(20,20), print_cmat=False):\n",
    "\n",
    "    y_pred = clf_GMM.predict(X_test,priors=priors)\n",
    "    acc_test = 100.0*skmetrics.accuracy_score(y_test, y_pred) \n",
    "    cmat = skmetrics.confusion_matrix(y_test,y_pred)\n",
    "    if(print_result):\n",
    "        print('Test Set:      Accuracy = %.2f%%'  % (acc_test) )\n",
    "    if(print_cmat):\n",
    "        pyspch.plot_confusion_matrix(cmat, labels=clf_GMM.classes,figsize=figsize)\n",
    "\n",
    "\n",
    "def train_test_GMM(X_train, X_test, y_train, y_test, classes, \n",
    "                   n_components=1, max_iter=20, tol=1.e-3,\n",
    "                   print_result=True, print_cmat=False):\n",
    "    clf_GM = GaussianMixtureClf(n_components, classes, max_iter, tol)\n",
    "    clf_GM.fit(X_train,y_train)\n",
    "    y_pred = clf_GM.predict(X_train)\n",
    "    acc_train = 100.0*skmetrics.accuracy_score(y_train, y_pred)\n",
    "    y_pred = clf_GM.predict(X_test)\n",
    "    acc_test = 100.0*skmetrics.accuracy_score(y_test, y_pred) \n",
    "    cmat = skmetrics.confusion_matrix(y_test,y_pred)\n",
    "    if(print_result):\n",
    "        lls, bics = llscore(clf_GM,X_train,y_train)\n",
    "        print('Training Set:  Accuracy = %.2f%%     LL = %.2f    BIC = %.2f ' % (acc_train,lls,bics) )\n",
    "        print('Test Set:      Accuracy = %.2f%%'  % (acc_test) )\n",
    "    if(print_cmat):\n",
    "        pyspch.plot_confusion_matrix(cmat, labels=classes)\n",
    "    return (acc_test,acc_train)\n",
    "\n",
    "def llscore(GMM,X,y):\n",
    "    ''' Average log likelihood per sample over the full data set (X,y) \n",
    "    and BIC per sample '''\n",
    "    ll = 0.\n",
    "    for k in range(0,GMM.n_classes) :\n",
    "        ll += GMM.gmm[k].score(X[y== GMM.classes[k],: ])\n",
    "    lls = ll.mean()\n",
    "    nparam = ((2*n_dim+1)*n_components -1 ) * GMM.n_classes\n",
    "    bics = -2*lls + (np.log(X.shape[0])* nparam) / float(X.shape[0])\n",
    "    return(lls,bics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GMM model\n",
    "n_dim = gmm_train_X.shape[1] # feature dimension\n",
    "n_components = 8 # number guassians, time = {8: 2min, 16: 3min, 32: 6min, 64: 12min}\n",
    "max_iter = 20 # max number EM iterations\n",
    "tolerance = 0.001 # tolerance (determines convergence)\n",
    "\n",
    "print(\"*** n_components=%d ***\" % (n_components))\n",
    "#_,_ = train_test_GMM(gmm_train_X, gmm_test_X, train_y, test_y, classes=labels,\n",
    "#                     n_components=n_components, max_iter=max_iter, tol=tolerance,\n",
    "#                     print_result=True, print_cmat=True)\n",
    "\n",
    "clf_GMM = train_GMM(gmm_train_X, train_y,  classes=labels,\n",
    "                     n_components=n_components, max_iter=max_iter, tol=tolerance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_GMM(gmm_test_X, test_y, clf_GMM, \n",
    "                     print_result=True, print_cmat=True, priors=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3eFPffdUGl_k"
   },
   "source": [
    "#### Questions\n",
    "\n",
    "**DNN model**\n",
    "\n",
    "Q1. Why use early stopping? What happens if you continue training?\n",
    "\n",
    "**GMM model**\n",
    "\n",
    "Q2. Does the GMM model's performance increase when using a window of frames as input feature, compared to a single frame, and why? \n",
    "\n",
    "**Comparison**\n",
    "\n",
    "Q2. In the end, is a DNN model better suited for phoneme classification as a GMM model, and why?\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ex_timit-3",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
