{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n5aNpi7xGPAI"
   },
   "source": [
    "# Frame  Classification\n",
    "\n",
    "A common use case is to recognize a single characteristic over an utterance, e.g. who is the speaker or is the speaker male or female (assuming you have a single speaker in the utterance).\n",
    "\n",
    "If we can do a rough estimate on a frame by frame basis , then we can integrate all that information over all frames.\n",
    "A simple method exists in treating all frame estimates as independent observations (iid assumption).  The iid. assumption is \n",
    "obviously incorrect in this case as the correlation over successive frames for speaker-id is very high.\n",
    "All things considered such naive method works well for classification. However, you should not believe the posterior probabilities, only the ordering in case of multiple classes.\n",
    "\n",
    "### CASE STUDY: Recognizing gender \n",
    "- atrribute: gender \\[m-f\\]\n",
    "- features: standard MFCC feature vectors\n",
    "- observations: a single utterance of a few secs\n",
    "- methodology:  \n",
    "    - training frame based classifiers using GMM's\n",
    "    - compute the utterance log-likelihood = sum of frame log-likelihood  (is correct under the assumption of iid frames)\n",
    "    - convert utterance log-likelihood to utterance posteriors (in the demo below we neglect priors, which is most often a valid choice)   \n",
    "    \n",
    "Similar scenarios can be developed for applications such as language, dialect, age, ... \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setting up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################\n",
    "### RUNNING THIS CELL FIRST ##########\n",
    "### will suppresses warnings on memory leaks, deprecation warnings and future warnings \n",
    "### It is brute force .  \n",
    "### Best is not to run it when you want to debug code or new installations\n",
    "import os, warnings \n",
    "os.environ[\"OMP_NUM_THREADS\"] = '2'  \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "####################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/compi1234/pyspch.git\n",
    "try:\n",
    "    import pyspch\n",
    "except ModuleNotFoundError:\n",
    "    try:\n",
    "        print(\n",
    "        \"\"\"\n",
    "        To enable this notebook on platforms as Google Colab, \n",
    "        install the pyspch package and dependencies by running following code:\n",
    "\n",
    "        !pip install git+https://github.com/compi1234/pyspch.git\n",
    "        \"\"\"\n",
    "        )\n",
    "    except ModuleNotFoundError:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "id": "I5lnCAVuGPAL",
    "outputId": "1fd27eb5-5d9d-433e-bfa7-2b9346550178"
   },
   "outputs": [],
   "source": [
    "# Importing Python's baseline machine learning stack \n",
    "#\n",
    "%matplotlib inline\n",
    "import sys,os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy.special import logsumexp\n",
    "from IPython.display import display,Audio\n",
    "\n",
    "# imports from the scikit-learn \n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn import metrics as skmetrics\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "import pyspch as Spch\n",
    "import pyspch.display as Spd\n",
    "import pyspch.sp as Sps\n",
    "from pyspch.stats import GMM\n",
    "\n",
    "# \n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"https://homes.esat.kuleuven.be/~spchlab/data/timit/\"\n",
    "def read_timit_corpus(root = \"https://homes.esat.kuleuven.be/~spchlab/data/timit/\",\n",
    "        name = \"conf/timit_train.corpus\"):\n",
    "    '''\n",
    "    reads part of the TIMIT corpus\n",
    "    '''\n",
    "    corpus_txt = Spch.read_txt(root+name)\n",
    "    corpus_list = []\n",
    "    for f in corpus_txt:\n",
    "        _,_,spkr,sent = f.split('/')\n",
    "        corpus_list.append([f,spkr[0],spkr,sent])\n",
    "    corpus = pd.DataFrame(corpus_list,columns=['filename','gender','spkr','sent-id'])\n",
    "    return(corpus)\n",
    "\n",
    "def get_features(root=\"https://homes.esat.kuleuven.be/~spchlab/data/timit/\",ftrdir=\"features/mel24/\",\n",
    "                  corpus=None,feature_args=None):\n",
    "    '''\n",
    "    gets preprocessed features, optionally postprocessed according to options the feature_extraction() module\n",
    "    '''\n",
    "    ftrs = []\n",
    "    for f in corpus.filename:\n",
    "        file = root + ftrdir + f + '.npy'\n",
    "        feature = np.load(Spch.read_fobj(file)) \n",
    "        if feature_args:\n",
    "            feature = Sps.feature_extraction(spg=feature, **feature_args)\n",
    "        ftrs.append(feature.T)\n",
    "    return(ftrs)\n",
    "\n",
    "def make_Xy(corpus=None,ftrs=None,attr='gender',downsample=1):\n",
    "    '''\n",
    "    make X,y feature vectors and classes\n",
    "    from ftrs and attribute 'attr' according to corpus\n",
    "    '''\n",
    "    \n",
    "    fnames = corpus['filename'].to_list()\n",
    "    X = None\n",
    "    y = None\n",
    "    for indx in range(len(corpus)):\n",
    "        Xi = ftrs[indx][::downsample,:]\n",
    "        yi = np.array([corpus.iloc[indx][attr]]*Xi.shape[0])\n",
    "        y = yi if y is None else np.concatenate((y,yi))\n",
    "        X = Xi if X is None else np.concatenate((X,Xi))\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKGg-uq-GPAT"
   },
   "source": [
    "### 2. The Database \n",
    "All experiments in this notebook are using (parts of) the TIMIT database\n",
    "\n",
    "##### TIMIT\n",
    "The TIMIT database contains well prepared speech recorded in mint acoustic conditions.\n",
    "The corpus is a mixture of a small set of sentences used by large groups of speakers and sentences that are different for each speaker.\n",
    "\n",
    "TIMIT is recorded in mint acoustic conditions and uses a sampling rate of 16 kHz.  \n",
    "Overall there are no long segments of noise/silence at beginning and end of the sentences; thus most of the data in an utterance is indeed speech data. \n",
    "For feature vectors we use MFCC-13, followed by (utterance length) mean-normalization.\n",
    "\n",
    "##### Importing and preprocessing the data\n",
    "- a. read the corpus files ; i.e. a list of filenames and meta data \n",
    "    + we use all speakers in the database but only a subset of the sentences per speaker\n",
    "    + train and test set are separate and as defined in the database\n",
    "- b. do feature extraction\n",
    "    + read precomputed mel filterbanks (24-D) \n",
    "    + postprocess with cepstral transformation and mean-norm\n",
    "\n",
    "**Note:** reading the features may take some  time, depending on the subsize of the corpus you are loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get a subset of the timit train corpus\n",
    "corpus_train = read_timit_corpus(name= \"conf/timit_train_si1.corpus\")\n",
    "ftrs_train = get_features(corpus=corpus_train, ftrdir = \"features/mel24/\", feature_args = {'n_cep':13,'Norm':'mean'} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a subset of the timit test corpus\n",
    "corpus_test = read_timit_corpus(name= \"conf/timit_test_si1.corpus\")\n",
    "ftrs_test = get_features(corpus=corpus_test, ftrdir = \"features/mel24/\", feature_args = {'n_cep':13,'Norm':'mean'} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN and TEST CORPUS STATISTICS\n",
      "Number of utterances in the train database:  462\n",
      "Number of utterances in the test database :  168\n",
      "NUmber of frames in the train database:      160008\n",
      "NUmber of frames in the test database:       58624\n"
     ]
    }
   ],
   "source": [
    "nframes_train = np.sum([ len(ftrs) for ftrs in ftrs_train] )\n",
    "nframes_test = np.sum([ len(ftrs) for ftrs in ftrs_test] )\n",
    "print(\"TRAIN and TEST CORPUS STATISTICS\")\n",
    "print(\"Number of utterances in the train database: \",len(ftrs_train))\n",
    "print(\"Number of utterances in the test database : \",len(ftrs_test))\n",
    "print(\"NUmber of frames in the train database:     \", nframes_train)\n",
    "print(\"NUmber of frames in the test database:      \", nframes_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. FRAME Classifier\n",
    "\n",
    "##### GMM Classifier\n",
    "GMM's are trained for each class that are then turned into a classifier by Bayes rule.   \n",
    "In the code below we ignore class priors.   \n",
    "The sequential nature of the data does not play any role in this setup as all frames (coming from same or different utterances/speakers) are treated as totally independent of each other.  Only the 'gender-attribute' is relevant.\n",
    "I.e. we treat successive feature vectors as being Independent Identically Distributed (IID) \n",
    "\n",
    "##### Implementation\n",
    "The data is structured as in sklearn with X as a collection of feature vectors and y the corresponding data labels.  \n",
    "The y-labels are inferred from the corpus meta-data given per file.\n",
    "\n",
    "We use the pyspch.stats.GMM class which implements a Gaussian Mixture classifier.  It is a wrapper around sklearn's GaussianMixture for multiple classes.\n",
    "\n",
    "**Note:**  You may get various warnings from sklearn, typically that some process/iteration did not converge. \n",
    "In a large GMM it  often happens that one or several of the components become degenerate.  These components should be fixed or dumped. \n",
    "This requires some heuristics that are not well worked out in sklearn.   Especially the K-Means initialization is fragile.\n",
    "Overall the harm is neglegible (except for cluttering your screen) and you may ignore these warnings, especially if recognition rates look adequate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train,y_train = make_Xy(corpus=corpus_train,ftrs=ftrs_train,attr='gender')\n",
    "X_test,y_test = make_Xy(corpus=corpus_test,ftrs=ftrs_train,attr='gender')\n",
    "classes = ['m','f']\n",
    "max_iter = 4\n",
    "n_components = 16\n",
    "tol = .001\n",
    "clf_GMM = GMM(classes=classes,n_components=n_components,max_iter=max_iter,tol=tol)\n",
    "clf_GMM.fit(X_train,y_train)\n",
    "#\n",
    "y_pred_train = clf_GMM.predict(X_train)\n",
    "print('Accuracy on train set: %.2f%%' % (100.0*skmetrics.accuracy_score(y_train, y_pred_train)))\n",
    "#\n",
    "y_pred_test = clf_GMM.predict(X_test)\n",
    "print('Accuracy on test set: %.2f%%' % (100.0*skmetrics.accuracy_score(y_test, y_pred_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. UTTERANCE Classifier \n",
    "\n",
    "##### From frames to utterances\n",
    "\n",
    "The frame classifier is turned into an utterance classifier.\n",
    "+ utterance likelihoods are computed by accumulating frame likelihoods\n",
    "    - this is mathematically correct if frames are i.i.d.  (independent and indentically distributed)\n",
    "    - this is methodologically acceptable as for the target classification task this is somewhat true\n",
    "+ utterance posteriors are obtained by normalizing such that the numbers sum up to one (no priors involved)\n",
    "    - the relative ranking of the posteriors is acceptable for classification decisions\n",
    "    - the absolute values of these posteriors are to be interpreted with a bucket of salt because the data is NOT iid\n",
    "    - in the gender experiment with (acoustically) well matching train and test we get utterance accuracies of 98%+ \n",
    "        \n",
    "        \n",
    "##### Implementation and numerical considerations\n",
    "\n",
    "- *utterance_classifier()* routine computes such utterance levels posteriors, it returns\n",
    "    + utterance level decision\n",
    "    + frame and utterance log-posteriors per class (per frame)\n",
    "    + total data likelihood per class (per frame)\n",
    "- implementation\n",
    "    + utterance likelihoods are the result of multilpying 100's of small numbers resulting inevitable in underflows\n",
    "    + therefore computations use as much as possible LOG-likelihoods and LOG-posteriors\n",
    "        + log-probs are conveniently added when MULTIPLYING is required\n",
    "        + we only convert log-probs to probs when an ADDITION is required\n",
    "    + We use following update formula for sentence likelihoods / posteriors:  utt_llh(t) = utt_llh(t-1) + floored_frame_llh(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utterance_classifier(ftrs,clf,floor=.01):\n",
    "    '''\n",
    "    get sentence scores for\n",
    "    - ftrs[]: a feature sequence\n",
    "    - clf   : a frame classifier with method predict_ftr_log_prob() defined\n",
    "    - floor : flooring applied to frame posteriors (before renormalization)\n",
    "    \n",
    "    returns:\n",
    "    - utt_prediction   : predicted class label for full utterance\n",
    "    - utt_posteriors   : utterance posteriors per class up to current frame\n",
    "    - utt_likelihood   : utterance log likelihood up to current frame\n",
    "    - frame_posteriors : class posteriors for current frame\n",
    "    '''\n",
    "    \n",
    "    log_floor = np.log(floor)\n",
    "    # frame log likelihoods\n",
    "    frame_llh = clf.predict_ftr_log_prob(ftrs)\n",
    "\n",
    "    nfr,nc = frame_llh.shape\n",
    "    utt_lik = np.zeros(nc)\n",
    "    frame_posteriors = np.zeros((nfr,nc))\n",
    "    utt_posteriors = 0.5*np.zeros((nfr,nc))\n",
    "    utt_likelihood = np.zeros(nfr)\n",
    "\n",
    "    for ifr in range(nfr):\n",
    "        frame_lik = frame_llh[ifr,:]\n",
    "        frame_tot = logsumexp(frame_lik)\n",
    "        frame_lik = np.maximum(frame_lik,frame_tot+log_floor)  # floor likelihood to fraction of total likelihood         \n",
    "        frame_tot = logsumexp(frame_lik)\n",
    "        frame_posteriors[ifr,:] = frame_lik - frame_tot        # renormalize \n",
    "        utt_lik += (frame_lik)\n",
    "        utt_total = logsumexp(utt_lik) \n",
    "        utt_likelihood[ifr] = utt_total\n",
    "        utt_posteriors[ifr,:] = utt_lik - utt_total\n",
    "        \n",
    "    utt_prediction = clf.classes[np.argmax(utt_posteriors[-1,:])]\n",
    "    return utt_prediction, utt_posteriors, utt_likelihood, frame_posteriors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# utterance classification results for train database\n",
    "corpus = corpus_train\n",
    "ftrs = ftrs_train\n",
    "errors = []\n",
    "max_length = -1\n",
    "for i in range(len(corpus)):\n",
    "    utt = ftrs[i][0:max_length,:]\n",
    "    pred_class, utt_posteriors, utt_likelihood,frame_posteriors = utterance_classifier(utt,clf_GMM,floor=0.01)\n",
    "    true_class = corpus.iloc[i]['gender'] \n",
    "    if pred_class != true_class:\n",
    "        errors.append((i,corpus.iloc[i]['filename'],true_class,pred_class))\n",
    "print(\"Train Corpus, #utterances: %d\" % len(corpus))\n",
    "print(\"Error Rate: %.2f %%\"% (100.*float(len(errors))/len(corpus)) )\n",
    "print(\"ERROR DETAILS: (utt_id, True Class, Recognized Class) \\n\",errors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence classification results for the test database\n",
    "corpus = corpus_test\n",
    "ftrs = ftrs_test\n",
    "errors = []\n",
    "max_length = -1\n",
    "for i in range(len(corpus)):\n",
    "    utt = ftrs[i][0:max_length,:]\n",
    "    pred_class, utt_posteriors, utt_likelihood,frame_posteriors = utterance_classifier(utt,clf_GMM,floor=0.01)\n",
    "    true_class = corpus.iloc[i]['gender'] \n",
    "    if pred_class != true_class:\n",
    "        errors.append((i,corpus.iloc[i]['filename'],true_class,pred_class))\n",
    "print(\"Test Corpus, #utterances: %d\" % len(corpus))\n",
    "print(\"Error Rate: %.2f %%\"% (100.*float(len(errors))/len(corpus)) )\n",
    "print(\"ERROR DETAILS:\\n\",errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_normalize(x):\n",
    "    '''\n",
    "    computes the geometric mean normalized (over time) version of log posteriors, i.e. \n",
    "        computes y[ifr,:] = x[ifr,:]/(ifr+1) and followed by renormalization\n",
    "    This is equivalent to taking the Nth-root of the likelihoods\n",
    "    \n",
    "    The reported numbers are more interpretable ; i.e. in the range of 0 ..1 (if converted to probs) with\n",
    "    rarely saturating values\n",
    "    '''\n",
    "    y = np.zeros(x.shape)\n",
    "    for ifr in range(x.shape[0]):\n",
    "        y[ifr,:] = np.divide(x[ifr,:],(ifr+1))\n",
    "    y = y - logsumexp(y,axis=1,keepdims=True)\n",
    "    return(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus = corpus_test\n",
    "ftrs = ftrs_test\n",
    "# examples with 'good recogniton' (5,100), and 'errors' (0: f->m, 41:m->f)\n",
    "for i in [5,100,0,41]:   \n",
    "    utt = ftrs[i]\n",
    "    pred_class, utt_posteriors, utt_likelihood,frame_posteriors = utterance_classifier(utt,clf_GMM,floor=0.001)\n",
    "    utt_postnorm = frame_normalize(utt_posteriors)\n",
    "    f,ax = plt.subplots(2,figsize=(10,4),sharex=True,gridspec_kw=dict(hspace=.3))\n",
    "    ax[0].plot(np.exp(frame_posteriors))\n",
    "    ax[0].set_title(\"utterance(%d) - frame posteriors\"%i)\n",
    "    ax[1].plot(np.exp(utt_postnorm))\n",
    "    ax[1].set_title(\" utterance posteriors (normalized for number of frames)\")\n",
    "    ax[1].set_ylim([0.,1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [53,54,101,140...]\n",
    "wavroot = \"https://homes.esat.kuleuven.be/~spchlab/data/timit/audio/\"\n",
    "corpus = corpus_test\n",
    "ftrs = ftrs_test\n",
    "for i in [1]:\n",
    "    utt = ftrs[i]\n",
    "    pred_class, utt_posteriors, utt_likelihood,frame_posteriors = utterance_classifier(utt,clf_GMM,floor=0.01)\n",
    "    fig = Spd.SpchFig(row_heights=[2,3,3,3])\n",
    "    fig.suptitle(\"UTTERANCE: %s  PREDICTION: %s\" % (corpus.iloc[i]['filename'], pred_class) )\n",
    "    wavname = wavroot + corpus.iloc[i]['filename'] + \".wav\"\n",
    "    try:\n",
    "        wavdata, sr = Spch.audio.load(wavname)\n",
    "        fig.add_line_plot(y=wavdata,iax=0,dx=1./sr)\n",
    "        display(Audio(data=wavdata,rate=sr))\n",
    "    except:\n",
    "    #    pass\n",
    "        fig.add_line_plot(utt[:,0],iax=0,dx=0.01)\n",
    "    utt_postnorm = frame_normalize(utt_posteriors)\n",
    "    #fig.add_line_plot(y=np.exp(frame_posteriors.T),iax=1,dx=0.01,ylabel=\"Frame Posteriors\")\n",
    "    ax = fig.get_axes()\n",
    "    ax[1].plot(np.exp(frame_posteriors))\n",
    "    ax[1].set_title(\"frame posteriors\")\n",
    "    ax[2].plot(utt_posteriors)\n",
    "    ax[2].set_title(\" utterance posteriors (log)\")\n",
    "    ax[3].plot(np.exp(utt_postnorm))\n",
    "    ax[3].set_title(\" utterance posteriors (normalized for number of frames)\")\n",
    "\n",
    "\n",
    "    #fig.add_line_plot(y=utt_posteriors.T,iax=2,dx=0.01,ylabel=\"Utt Posteriors (log)\")\n",
    "    #fig.add_line_plot(y=np.exp(utt_postnorm.T),iax=3,dx=0.01,ylabel=\"Norm Utt Posteriors\")\n",
    "    #ax[1].plot(frame_posteriors)\n",
    "    #ax[2].plot(utt_posteriors)\n",
    "    fig.axes[3].legend(clf_GMM.classes)\n",
    "    #ax[1].plot(np.exp(frame_posteriors))\n",
    "    #ax[2].plot(np.exp(utt_posteriors))\n",
    "    display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "hillenbrand-2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
