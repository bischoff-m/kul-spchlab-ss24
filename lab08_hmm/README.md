# LAB08:  Hidden Markov Models - Basic Algorithms



## Background

For some background reading and course note examples, cfr:   
- hmm_examples_discrete.ipynb
- hmm_examples_gauss1.ipynb
- hmm_examples_gauss2.ipynb


## Exercise1 - Computing with HMMs: basics

The purpose of this exercise is do basic computations, such as Viterbi alignment, with a given HMM.

The HMMs used are very simple, but as you will experience, they get involved quite quickly.
Doing these BY HAND will help you in understanding how the problem gets manageable by splitting the observation probabilities out of trellis computations.  


## Exercise2 - Computing with HMMs: Understanding the sequence modeling and feature transformations

HMMs are great tools for sequence modeling. 
The concept is that your data is generated by a sequence of states with different properties.
By doing a Viterbi alignment we can track through which states the data moved and we get a state level transcription
that can be understood.

However, there are some important limitations to the HMM concept as well.  
A state models a stationary process defined by the observation model.  
Within a state data is assumed to be IID (Independent Identically Distributed).
This implies that the order of the data WITHIN a state is irrelevant.
Thus the likelihood of data sequence x1-x2-x3 being observed in S1 is identical to sequence x3-x2-x1 being observed.  

Exercise 2 will draw your attention to this stationarity assumption in a number of different ways:
- how very different "looking" data can have identical scores on an HMM
- how very different "looking" training data can lead to identical models

Finally we will show how sequence information can simply be incorporated in the data 
by the use of delta features.

This exercise CAN be done by hand but involves quite a few computations.
You can also track it in the notebook and do a few computations.  
However, try to answer each of the questions as you progress and without looking ahead !!


## Exercise3 - Recognition with HMMs: