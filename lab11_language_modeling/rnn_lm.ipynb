{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/compi1234/spchlab/blob/master/RNN_LM/rnn_lm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FyAmshu3bFz6"
   },
   "source": [
    "# Language Modeling with Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0fc7h-VVbPBV"
   },
   "source": [
    "In this notebook, we will see how you can train a recurrent neural network language model.\n",
    "\n",
    "We will start by importing TensorFlow, which is Google's open-source library for machine learning. Next, we will explain how to do data processing for language modeling and explain the most important classes and function that we will use. We will give a short introduction to word embeddings, testing a network and the importance of hyperparameters during training.\n",
    "\n",
    "Dependencies:\n",
    "- Tensorflow: This notebook uses tensorflow.compat.v1.  This does generate a number of deprecation WARNINGS in COLAB. This is normal and no action is required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vzV_zLwTbr94"
   },
   "source": [
    "## Importing TensorFlow and other requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "esuUK2ev_Nyt"
   },
   "source": [
    "We start by importing TensorFlow and checking if we are running on GPU. You will probably be asked two confirmations; click YES twice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_yKJ-Plo_HTS",
    "outputId": "e53a29c4-b88a-4e7e-e65d-e79c2333b921"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-31cd2527c4f4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisable_v2_behavior\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdevice_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgpu_device_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdevice_name\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'/device:GPU:0'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1FKKFmkwat9W"
   },
   "source": [
    "If the code above raised an error, you should make sure that you are using a GPU in the following way: select 'Runtime' in the top bar, then 'Change runtime type' and choose 'GPU' as hardware accelerator. Training neural networks is much faster on a GPU (graphics processing unit) than on a CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OZ6KKnfoahTc"
   },
   "source": [
    "Next, we import the other standard Python packages that we need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BnEd3YIyDsDd"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import urllib, collections, os\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ILImWZp-_JCI"
   },
   "source": [
    "In the 3 cells below we are defining some classes and functions that we will use throughout the notebook. You have to run the cells to make sure you can use them, but you do not have to look at the details in the code. By default we have hidden this code, but you may click on SHOW CODE to visualize each of them (batchGenerator.py, rnn_lm.py and run_lm.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bAsnD-TM6qVZ"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "class batchGenerator(object):\n",
    "  '''\n",
    "  This class generates batches for a dataset.\n",
    "  Input arguments:\n",
    "    data: list of indices (word ids)\n",
    "    batch_size: number of sequences in a mini-batch\n",
    "    num_steps: length of each sequence in the mini-batch\n",
    "    test: boolean, is True if we are testing; in that case batch_size and num_steps are 1\n",
    "  '''\n",
    "  \n",
    "  def __init__(self, data, batch_size=32, num_steps=50, test=False):\n",
    "    '''\n",
    "    Prepares a dataset.\n",
    "    '''\n",
    "    self.batch_size = batch_size\n",
    "    self.num_steps = num_steps\n",
    "    self.test = test \n",
    "\n",
    "    self.data_array = np.array(data)\n",
    "  \n",
    "    if not self.test:\n",
    "      len_batch_instance = int(len(data) / batch_size)\n",
    "\n",
    "      print(int(batch_size*len_batch_instance))\n",
    "      data_array = self.data_array[:batch_size*len_batch_instance]\n",
    "\n",
    "      # divide data in batch_size parts\n",
    "      self.data_reshaped = np.reshape(data_array, (batch_size, len_batch_instance))\n",
    "\n",
    "      # number of mini-batches that can be generated\n",
    "      self.num_batches_in_data = len_batch_instance / num_steps - 1\n",
    "    \n",
    "    self.curr_idx = 0\n",
    "  \n",
    "  def generate(self):\n",
    "    '''\n",
    "    Generates\n",
    "      input_batch: numpy array (batch_size x num_steps) or None, if the end of the dataset is reached\n",
    "      target_batch: numpy array (batch_size x num_steps) or None, if the end of the dataset is reached\n",
    "      end_reached: boolean, True is end of dataset is reached\n",
    "    '''\n",
    "    \n",
    "    if self.test:\n",
    "      if self.curr_idx+1 >= len(self.data_array):\n",
    "        return None, None, True\n",
    "      \n",
    "      input_batch = [[self.data_array[self.curr_idx]]]\n",
    "      target_batch = [[self.data_array[self.curr_idx+1]]]\n",
    "      \n",
    "    else:\n",
    "      if self.curr_idx >= self.num_batches_in_data:\n",
    "        return None, None, True\n",
    "\n",
    "      # input: take slice of size \n",
    "      input_batch = self.data_reshaped[:,self.curr_idx*self.num_steps:self.curr_idx*self.num_steps+self.num_steps]\n",
    "\n",
    "      # target = input shifted 1 time step\n",
    "      target_batch = self.data_reshaped[:,self.curr_idx*self.num_steps+1:self.curr_idx*self.num_steps+self.num_steps+1]\n",
    "\n",
    "    self.curr_idx += 1\n",
    "    \n",
    "    return input_batch, target_batch, False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aHDgLlvV6sfc"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "class rnn_lm(object):\n",
    "  '''\n",
    "  This is a class to build and execute a recurrent neural network language model.\n",
    "  Arguments:\n",
    "    cell: type of RNN cell (only LSTM is currently implemented)\n",
    "    optimizer: 'SGD' or 'Adam'\n",
    "    lr: learning rate\n",
    "    vocab_size: size of the vocabulary\n",
    "    embedding_size: size of continuous embedding that will be input to the RNN\n",
    "    hidden_size: size of the hidden layer\n",
    "    dropout rate: value between 0 and 1, number of neurons that will be \n",
    "        kept (not dropped) during training, prevents overfitting\n",
    "    batch_size: number of sequences that will be input simultaneously\n",
    "    num_steps: length of each input sequence = number of steps in backpropagation through time\n",
    "    is_training: boolean, True is we want to update the parameters of the model\n",
    "  '''\n",
    "  \n",
    "  def __init__(self,\n",
    "              cell='LSTM',\n",
    "              optimizer='Adam',\n",
    "              lr=0.01,\n",
    "              vocab_size=10000,\n",
    "              embedding_size=64,\n",
    "              hidden_size=128,\n",
    "              dropout_rate=0.5,\n",
    "              batch_size=32,\n",
    "              num_steps = 50,\n",
    "              is_training=True):\n",
    "    # hyperparameters that can be changed\n",
    "    self.which_cell = cell\n",
    "    self.which_optimizer = optimizer\n",
    "    self.vocab_size = vocab_size\n",
    "    self.embedding_size = embedding_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.dropout_rate = dropout_rate\n",
    "    self.is_training = is_training\n",
    "    self.lr = lr\n",
    "    self.batch_size = batch_size\n",
    "    self.num_steps = num_steps\n",
    "    \n",
    "    # hard-coded hyperparameters\n",
    "    self.max_grad_norm = 5\n",
    "    \n",
    "    self.init_graph()\n",
    "    \n",
    "    self.output, self.state = self.feed_to_network()\n",
    "    \n",
    "    self.loss = self.calc_loss(self.output)\n",
    "    \n",
    "    if self.is_training:\n",
    "      self.update_params(self.loss)\n",
    "    \n",
    "    \n",
    "  def init_graph(self):\n",
    "    '''\n",
    "    This function initializes all elements of the network.\n",
    "    '''\n",
    "    \n",
    "    self.inputs = tf.placeholder(dtype=tf.int32, shape=[self.batch_size, self.num_steps])\n",
    "    self.targets = tf.placeholder(dtype=tf.int32, shape=[self.batch_size, self.num_steps])\n",
    "    \n",
    "    # input embedding weights\n",
    "    self.embedding = tf.get_variable(\"embedding\", \n",
    "                                     [self.vocab_size, self.embedding_size], \n",
    "                                     dtype=tf.float32)\n",
    "    \n",
    "    # hidden layer\n",
    "    if self.which_cell == 'LSTM':\n",
    "      self.basic_cell = tf.nn.rnn_cell.LSTMCell(self.hidden_size)\n",
    "    elif self.which_cell == 'RNN':\n",
    "      self.basic_cell = tf.nn.rnn_cell.BasicRNNCell(self.hidden_size)\n",
    "    else:\n",
    "      raise ValueError(\"Specify which type of RNN you want to use: RNN or LSTM.\")\n",
    "      \n",
    "    # apply dropout  \n",
    "    self.cell = tf.nn.rnn_cell.DropoutWrapper(self.basic_cell, \n",
    "                                              output_keep_prob=self.dropout_rate)\n",
    "    \n",
    "    # initial state contains all zeros\n",
    "    self.initial_state = self.cell.zero_state(self.batch_size, tf.float32)\n",
    "    \n",
    "    # output weight matrix and bias\n",
    "    self.softmax_w = tf.get_variable(\"softmax_w\",\n",
    "                                     [self.hidden_size, self.vocab_size], \n",
    "                                     dtype=tf.float32)\n",
    "    self.softmax_b = tf.get_variable(\"softmax_b\",\n",
    "                                     [self.vocab_size], \n",
    "                                     dtype=tf.float32)\n",
    "    \n",
    "    self.initial_state = self.cell.zero_state(self.batch_size, dtype=tf.float32)\n",
    "    \n",
    "    \n",
    "  def feed_to_network(self):\n",
    "    '''\n",
    "    This function feeds the input to the network and returns the output and the state.\n",
    "   \n",
    "    '''\n",
    "    \n",
    "    # map input indices to continuous input vectors\n",
    "    inputs = tf.nn.embedding_lookup(self.embedding, self.inputs)\n",
    "\n",
    "\t  # use dropout on the input embeddings\n",
    "    inputs = tf.nn.dropout(inputs, self.dropout_rate)\n",
    "    \n",
    "    state = self.initial_state\n",
    "    \n",
    "    # feed inputs to network: outputs = predictions, state = new hidden state\n",
    "    outputs, state = tf.nn.dynamic_rnn(self.cell, inputs, sequence_length=None, initial_state=state)\n",
    "    \n",
    "    output = tf.reshape(tf.concat(outputs, 1), [-1, self.hidden_size])\n",
    "    \n",
    "    return output, state\n",
    "    \n",
    "  \n",
    "  def calc_loss(self, output):\n",
    "    \n",
    "    # calculate logits\n",
    "    # shape of logits = [batch_size*num_steps, vocab_size]\n",
    "    logits = tf.matmul(output, self.softmax_w) + self.softmax_b\n",
    "    \n",
    "    self.softmax = tf.nn.softmax(logits)\n",
    "      \n",
    "    # calculate cross entropy loss\n",
    "    # reshape targets such that it has shape [batch_size*num_steps]\n",
    "    # loss: contains loss for every time step in every batch\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=tf.reshape(self.targets, [-1]), logits=logits)\n",
    "      \n",
    "    # average loss per batch\n",
    "    avg_loss = tf.reduce_sum(loss) / self.batch_size\n",
    "    \n",
    "    return avg_loss\n",
    "  \n",
    "  def update_params(self, loss):\n",
    "    \n",
    "    # calculate gradients for all trainable variables \n",
    "    # + clip them if their global norm > 5 (prevents exploding gradients)\n",
    "    grads, _ = tf.clip_by_global_norm(\n",
    "        tf.gradients(loss, tf.trainable_variables()), self.max_grad_norm)\n",
    "    \n",
    "    if self.which_optimizer == 'SGD':\n",
    "      optimizer = tf.train.GradientDescentOptimizer(self.lr)\n",
    "    elif self.which_optimizer == 'Adam':\n",
    "      optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "    else:\n",
    "      raise ValueError(\"Specify which type of optimizer you want to use: SGD or Adam.\")\n",
    "    \n",
    "    # update the weights\n",
    "    self.train_op = optimizer.apply_gradients(\n",
    "\t\t\t\tzip(grads, tf.trainable_variables()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ABVaoeuv6xdY"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "def run_lm(name='LSTM', cell='LSTM', \n",
    "           optimizer='Adam', lr=0.01, \n",
    "           vocab_size = 10000, embedding_size=64, \n",
    "           hidden_size=128, dropout_rate=0.5, \n",
    "           num_steps=50, inspect_emb=False, \n",
    "           train_ids=None, valid_ids=None, \n",
    "           test_ids=None, test_log_prob=False):\n",
    "  '''\n",
    "  Creates training, validation and/or test models,\n",
    "  trains, validates and/or tests the model.\n",
    "  Arguments:\n",
    "    name: name that will be used to save the model\n",
    "    cell: type of RNN cell (only LSTM is currently implemented)\n",
    "    optimizer: 'SGD' or 'Adam'\n",
    "    lr: learning rate\n",
    "    vocab_size: size of the vocabulary\n",
    "    embedding_size: size of continuous embedding that will be input to the RNN\n",
    "    hidden_size: size of the hidden layer\n",
    "    dropout rate: value between 0 and 1, number of neurons that will be \n",
    "        kept (not dropped) during training, prevents overfitting\n",
    "    num_steps\n",
    "    inspect_emb: boolean, if True we want to return the embedding_matrix\n",
    "    train_ids: training data\n",
    "    valid_ids: validation data\n",
    "    test_ids: test data\n",
    "    test_log_prob: boolean, if True we only want to test the log probability for a test sentence\n",
    "  '''\n",
    "    \n",
    "  with tf.Graph().as_default() as graph:\n",
    "\n",
    "      # create the models\n",
    "      if not test_log_prob:\n",
    "      \n",
    "        with tf.variable_scope(\"Model\"):\n",
    "          rnn_train = rnn_lm(cell=cell,\n",
    "                             optimizer=optimizer, \n",
    "                             lr=lr,\n",
    "                             vocab_size=vocab_size,\n",
    "                             embedding_size=embedding_size,\n",
    "                             hidden_size=hidden_size,\n",
    "                             dropout_rate=dropout_rate)\n",
    "\n",
    "          saver = tf.train.Saver()\n",
    "\n",
    "        with tf.variable_scope(\"Model\", reuse=True):\n",
    "          rnn_valid = rnn_lm(cell=cell, \n",
    "                             optimizer=optimizer,\n",
    "                             lr=lr,\n",
    "                             vocab_size=vocab_size, \n",
    "                             embedding_size=embedding_size,\n",
    "                             hidden_size=hidden_size,\n",
    "                             dropout_rate=dropout_rate,\n",
    "                             is_training=False)\n",
    "          \n",
    "        reuse = True\n",
    "        \n",
    "      else:\n",
    "        reuse = False\n",
    "               \n",
    "      with tf.variable_scope(\"Model\", reuse=reuse):\n",
    "        rnn_test = rnn_lm(cell=cell, \n",
    "                           optimizer=optimizer, \n",
    "                           lr=lr,\n",
    "                           vocab_size=vocab_size,\n",
    "                           embedding_size=embedding_size,\n",
    "                           hidden_size=hidden_size,\n",
    "                           dropout_rate=dropout_rate,\n",
    "                           batch_size=1,\n",
    "                           num_steps=1,\n",
    "                           is_training=False)\n",
    "      \n",
    "\n",
    "      sv = tf.train.Supervisor(logdir=name)\n",
    "\n",
    "      with sv.managed_session(config=tf.ConfigProto()) as session:\n",
    "        \n",
    "        if not test_log_prob:\n",
    "        \n",
    "          for i in range(5):\n",
    "\n",
    "            print('Epoch {0}'.format(i+1))\n",
    "\n",
    "            train_ppl = run_epoch(session, rnn_train, train_ids, num_steps=num_steps)\n",
    "            print('Train perplexity: {0}'.format(train_ppl))\n",
    "\n",
    "            valid_ppl = run_epoch(session, rnn_valid, valid_ids, num_steps=num_steps, is_training=False)\n",
    "            print('Validation perplexity: {0}'.format(valid_ppl))\n",
    "\n",
    "          save_path = saver.save(session, \"{0}/rnn.ckpt\".format(name))\n",
    "          print('Saved the model to ',save_path)\n",
    "\n",
    "        test_ppl = run_epoch(session, rnn_test, test_ids, num_steps=num_steps,\n",
    "                             is_training=False, is_test=True, \n",
    "                             test_log_prob=test_log_prob)\n",
    "        if not test_log_prob:\n",
    "          print('Test perplexity: {0}'.format(test_ppl))\n",
    "        \n",
    "        if inspect_emb: \n",
    "          emb_matrix = tf.get_default_graph().get_tensor_by_name(\"Model/embedding:0\")\n",
    "          emb_matrix_np = emb_matrix.eval(session=session)\n",
    "\n",
    "          return emb_matrix_np\n",
    "\n",
    "        else:\n",
    "\n",
    "          return None\n",
    "\n",
    "def run_epoch(session, rnn, data, num_steps=50, is_training=True, is_test=False, test_log_prob=False):\n",
    "    '''\n",
    "    This function runs a single epoch (pass) over the data,\n",
    "    updating the model parameters if we are training,\n",
    "    and returns the perplexity.\n",
    "    Input arguments:\n",
    "      rnn: object of the rnn_lm class\n",
    "      data: list of word indices\n",
    "      num_steps\n",
    "      is_training: boolean, True is we are training the model\n",
    "      is_test: boolean, True is we are testing a trained model\n",
    "      test_log_prob: boolean, True if we want the log probability\n",
    "    Returns:\n",
    "      ppl: float, perplexity of the dataset\n",
    "    '''\n",
    "  \n",
    "    generator = batchGenerator(data, test=is_test)\n",
    "      \n",
    "    state = session.run(rnn.initial_state)\n",
    "    sum_loss = 0.0\n",
    "    iters = 0\n",
    "    \n",
    "    if test_log_prob: \n",
    "      sum_log_prob = 0.0\n",
    "      \n",
    "    while True:\n",
    "\n",
    "      input_batch, target_batch, end_reached = generator.generate()\n",
    "        \n",
    "      if end_reached:\n",
    "        break\n",
    "\n",
    "      feed_dict = {rnn.inputs: input_batch,\n",
    "                  rnn.targets: target_batch,\n",
    "                  rnn.initial_state : state}\n",
    "\n",
    "      fetches = {'loss': rnn.loss,\n",
    "                'state': rnn.state}\n",
    "      \n",
    "      if is_training:\n",
    "        fetches['train_op'] = rnn.train_op\n",
    "        \n",
    "      if test_log_prob:\n",
    "        fetches['softmax'] = rnn.softmax\n",
    "        \n",
    "      result = session.run(fetches, feed_dict)\n",
    "        \n",
    "      state = result['state']\n",
    "      loss = result['loss']\n",
    "      \n",
    "      if test_log_prob:\n",
    "        softmax = result['softmax']\n",
    "        prob_target = softmax[0][target_batch[0][0]]\n",
    "        sum_log_prob += np.log(prob_target)\n",
    "\n",
    "      sum_loss += loss\n",
    "      # the loss is an average over num_steps\n",
    "      if is_test:\n",
    "        iters += 1\n",
    "      else:\n",
    "        iters += num_steps\n",
    "        \n",
    "    # calculate perplexity    \n",
    "    ppl = np.exp(sum_loss / iters)\n",
    "    \n",
    "    if test_log_prob:\n",
    "      print('Log probability: {0}'.format(sum_log_prob))\n",
    "    \n",
    "    return ppl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o3-bMavvngQx"
   },
   "source": [
    "If you have run all cells in this section, you can now start the following section on data processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7hmCKPx4KEcG"
   },
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-oKtBwJQAJED"
   },
   "source": [
    "We will train our language models on **Penn TreeBank**, which is a publicly available benchmark dataset. A benchmark dataset can be used to easily compare models, since everyone has access to the same data. Many published papers use Penn TreeBank as dataset.\n",
    "\n",
    "Penn TreeBank consists of among others newspaper articles, transcribed telephone conversations and manuals. The training set contains ca. 900.000 words, the validation set ca. 70.000 words and the test set ca. 80.000 words. This is a very small dataset (nowadays language models can be trained on billions of words), but it is large enough for our purposes.\n",
    "\n",
    "We now download the training, validation and test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t6AIeiR2BaMm"
   },
   "outputs": [],
   "source": [
    "train_url = 'http://homes.esat.kuleuven.be/~spchlab/data/penntreebank/train.txt'\n",
    "valid_url = 'http://homes.esat.kuleuven.be/~spchlab/data/penntreebank/valid.txt'\n",
    "test_url = 'http://homes.esat.kuleuven.be/~spchlab/data/penntreebank/test.txt'\n",
    "train_file = urllib.request.urlopen(train_url).read()\n",
    "valid_file = urllib.request.urlopen(valid_url).read()\n",
    "test_file = urllib.request.urlopen(test_url).read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Ye3oJavH-8F"
   },
   "source": [
    "The data looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GXEcoaWKIAtE",
    "outputId": "536bfbc0-9756-48fb-ddc6-052f8c07fd48"
   },
   "outputs": [],
   "source": [
    "print('{0}...'.format(valid_file[:500]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ZF1PoUJDRkH"
   },
   "source": [
    "The data has been **normalized**: all words not in the vocabulary are mapped to an unknown words class (<unk\\>), all numbers are mapped to the 'N' class, each line contains a single sentence, punctuation has been removed, and so on. \n",
    "\n",
    "The purpose of normalization is among others to get rid of all information that is not necessary (such as punctuation), to solve redundancies (for example the same word can occur with different spellings, e.g. 'normalisation' or 'normalization', and we want to get rid of such variants) and to make sure the language model will be able to generalize better. An example of the latter case is the mapping of all numbers to 'N':  in the example above, 'in N years', 'N' can correspond to any number. Assume that in our training data, we see 'in 20 years' and 'in 11 years', and in our test data, we see 'in 5 years'. If '20', '11' and '5' are not mapped to 'N', we have never seen 'in 5 years' before, and the probability estimate for 'in 5 years' will be worse.\n",
    "  \n",
    "We will now read the data, add end-of-sentence symbols <eos\\> ( since we want to be able to predict the end of a sentence too), and count the frequency of every word in the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y531EV5aDVd0"
   },
   "outputs": [],
   "source": [
    "# convert the string to a list and replace newlines with the end-of-sentence symbol <eos>\n",
    "# ignore empty elements ''\n",
    "train_text = [w for w in train_file.decode(encoding='utf-8', errors='strict').replace('\\n',' <eos>').split(' ') if w != '']\n",
    "valid_text = [w for w in valid_file.decode(encoding='utf-8', errors='strict').replace('\\n',' <eos>').split(' ') if w != '']\n",
    "test_text = [w for w in test_file.decode(encoding='utf-8', errors='strict').replace('\\n',' <eos>').split(' ') if w != '']\n",
    "\n",
    "# count the frequencies of the words in the training data\n",
    "counter = collections.Counter(train_text)\n",
    "\n",
    "# sort according to decreasing frequency\n",
    "count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5jkJPz5CrDFq",
    "outputId": "6e9af3ce-2c24-4e3c-9bb7-f10c88afe5e2"
   },
   "outputs": [],
   "source": [
    "# make a list of the 10000 most frequent words , we will use this later\n",
    "most_frequent_words = []\n",
    "for i in range(0,10000):\n",
    "  most_frequent_words.append(count_pairs[i][0])\n",
    "# just print a selection if you want to\n",
    "most_frequent_words[100:105]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ci3iri5qAu2n"
   },
   "source": [
    "We can take a look at the counts of the words in the training set, and compare them with the counts of the words in the validation set. Let' take a look at the top 20 most frequent words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8AC1Y_TxAHYK",
    "outputId": "7b703bb8-f698-489d-eb2d-c9614bb91245"
   },
   "outputs": [],
   "source": [
    "# count the frequencies of the words in the validation data\n",
    "counter_valid = collections.Counter(valid_text)\n",
    "\n",
    "# sort according to decreasing frequency\n",
    "count_pairs_valid = sorted(counter_valid.items(), key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "fltrain = float(len(train_text))\n",
    "flvalid = float(len(valid_text))\n",
    "\n",
    "print('Top 20 most frequent words:')\n",
    "print('Train (freq.)\\t\\tValid (freq.)')\n",
    "# we can take a look a the 20 most frequent words + their frequencies:\n",
    "for i in range(20):\n",
    "  freq_train = round((float(count_pairs[i][1]) / fltrain)*100,3)\n",
    "  freq_valid = round((float(count_pairs_valid[i][1]) / flvalid)*100,3)\n",
    "  \n",
    "  print('{0} ({1} - {2}%)\\t\\t{3} ({4} - {5}%)'.format(\n",
    "      count_pairs[i][0], count_pairs[i][1], freq_train,\n",
    "      count_pairs_valid[i][0], count_pairs_valid[i][1], freq_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wCHz8MhRBvoQ"
   },
   "source": [
    "Between brackets, we print the raw counts followed by the relative frequencies. \n",
    "\n",
    "*   Which types of words are the most frequent?\n",
    "*   How do you explain the difference in raw counts between the training text and validation text?\n",
    "*   How do you explain the difference in relative frequencies between the training text and validation text?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mOjJpJHTcbDl"
   },
   "source": [
    "Let's now take a look at the mid- and low-frequency range. In the cell below, we look at the relative frequencies for a few words starting at begin_idx. You can change this value to inspect other words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0lwxGw9Ap_i",
    "outputId": "45c5e0f7-89e9-454e-da8e-08741ce4cbef"
   },
   "outputs": [],
   "source": [
    "begin_idx = 200 # you can change this value\n",
    "end_idx = begin_idx + 20\n",
    "\n",
    "print('Word\\t\\tTrain freq.\\tValid freq.')\n",
    "for i in range(begin_idx, end_idx):\n",
    "  entry = count_pairs[i][0]\n",
    "  \n",
    "  freq_train = round((float(counter[entry]) / fltrain)*100,3)\n",
    "  freq_valid = round((float(counter_valid[entry]) / flvalid)*100,3)\n",
    "  \n",
    "  print('{0}\\t\\t{1}%\\t\\t{2}%'.format(\n",
    "      entry, freq_train, freq_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_jb9hELmchMW"
   },
   "source": [
    "\n",
    "\n",
    "*   Are the differences in relative frequencies larger for the mid-frequency range than for the high-frequency range?\n",
    "*   How do you explain this?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z5Fok7dfA6L9"
   },
   "source": [
    "We now create a mapping from words to indices. The real input for the neural network will be indices, because they take up less space and because it makes certain operations easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PGMZqJFVAfoh"
   },
   "outputs": [],
   "source": [
    "# words = list of all the words (in decreasing frequency)\n",
    "items, _ = list(zip(*count_pairs))\n",
    "\n",
    "# make a dictionary with a mapping from each word to an id; word with highest frequency gets lowest id etc.\n",
    "item_to_id = dict(zip(items, range(len(items))))\n",
    "id_to_item = dict(zip(range(len(items)), items))\n",
    "vocab_size = len(item_to_id)\n",
    "\n",
    "# convert the words to indices\n",
    "train_ids_large = [item_to_id[item] for item in train_text]\n",
    "valid_ids_large = [item_to_id[item] for item in valid_text]\n",
    "test_ids_large = [item_to_id[item] for item in test_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-6KQShlZFcGx"
   },
   "source": [
    "Once the data is converted to ids, it looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vlo5SpAHFhLq",
    "outputId": "0c26e250-6071-4e57-cdea-96556e37404b"
   },
   "outputs": [],
   "source": [
    "print('Here is an example of words and their indices:')\n",
    "for i in range(40):\n",
    "  print('{0}\\t{1}'.format(valid_text[i], valid_ids_large[i]))\n",
    "print('\\nAnd this is what the input looks like, a list of indices:')\n",
    "print(valid_ids_large[:40])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MR026sYjd3sO"
   },
   "source": [
    "To speed up some experiments, we take a subset of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kom3XmMHdxQt",
    "outputId": "c04a37c7-d9be-4e56-b05c-819b1d2e9694"
   },
   "outputs": [],
   "source": [
    "# take a smaller subset to speed up training\n",
    "train_ids = train_ids_large[:50000]\n",
    "valid_ids = valid_ids_large[:10000]\n",
    "test_ids = test_ids_large[:10000]\n",
    "\n",
    "#train_ids = [int(i) for i in train_ids]\n",
    "#valid_ids = [int(i) for i in valid_ids]\n",
    "#test_ids = [int(i) for i in test_ids]\n",
    "\n",
    "print('Number of words in small training set: {0}'.format(len(train_ids)))\n",
    "print('Number of words in small validation set: {0}'.format(len(valid_ids)))\n",
    "print('Number of words in small test set: {0}'.format(len(test_ids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FIhqMJ2CKMxP"
   },
   "source": [
    "## The classes and functions that we will use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TuNJPpA-oZRs"
   },
   "source": [
    "We will now explore the classes and functions that we have defined earlier for training and testing our language models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jBnJVfPuNzmI"
   },
   "source": [
    "The class for an RNN language model is **rnn_lm()**. We will see later which options we can use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tIHeGqTCW8J-"
   },
   "source": [
    "**batchGenerator(<dataset\\>)** is class that will generate mini-batches from the data. <dataset\\> is a list of word ids."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_SB0UTRdnU5"
   },
   "source": [
    "batchGenerator is a class that will iterate over the data set and create **mini-batches** that will be the input for the neural network. A mini-batch contains several sentences/word sequences, and feeding mini-batches instead of a single sentence or a single word to the network speeds up the processing, and also causes better convergence of the model.\n",
    "\n",
    "The batches are matrices of the size **batch_size** x **num_steps**. Batch_size is the number of different sequences in a single batch, and num_steps the length of each  sequence.\n",
    "\n",
    "Here is an example of how batchGenerator can be used. You will notice that the target batch contains the same indices as the input batch, but shifted one (time) step to the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qHiY_eJ0dpUa",
    "outputId": "a2f5154a-b7ef-4919-ce77-40481c15e125"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_steps = 50\n",
    "\n",
    "generator = batchGenerator(valid_ids, batch_size=batch_size, num_steps=num_steps)\n",
    "input_batch, target_batch, end_reached = generator.generate()\n",
    "print('Shape of the mini-batch: {0}'.format(input_batch.shape))\n",
    "print('This is what an input batch looks like:\\n{0}'.format(input_batch))\n",
    "print('And this is what a target batch looks like:\\n{0}'.format(target_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ibvCjKOJI9h"
   },
   "source": [
    "Here is a function which pretty-prints what the mini-batches look like. You can give it a batch as first argument, and as second argument the index that you want to look at. In our case, there are 32 sequences in every mini-batch, so the indices range between 0 and 31 (in Python, indices always start at 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-hMxzS8dHC-j"
   },
   "outputs": [],
   "source": [
    "def print_batch(batch, idx):\n",
    "  for i in range(num_steps):\n",
    "      word = id_to_item[batch[idx][i]]\n",
    "      if word == '<eos>':\n",
    "         print()\n",
    "      else:\n",
    "        print(word, end=' ')\n",
    "  print()\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q0RGWHE1Jnvv"
   },
   "source": [
    "And here are some examples of what the first and fourth sequence of the  input and target batch look like. Try it yourself with some new values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oG3sipRFJh7H",
    "outputId": "3af3a3b1-d0b2-4faa-f952-c97824da6fe2"
   },
   "outputs": [],
   "source": [
    "print_batch(input_batch, 0)\n",
    "print_batch(target_batch, 0)\n",
    "\n",
    "print_batch(input_batch, 3)\n",
    "print_batch(target_batch, 3)\n",
    "\n",
    "# try it yourself:\n",
    "# print_batch(..., ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cf7tQcgTc8RS"
   },
   "source": [
    "\n",
    "\n",
    "*   What is the difference between the input batch and the target batch?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CY4KxCsUenOy"
   },
   "source": [
    "**run_lm():** this function can be called to build, train and test models with different parameter settings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vBbmu8MyeRjg"
   },
   "source": [
    "**run_epoch()**: this is a function that does one pass over the whole dataset. If we are training the model, it will update the parameters and return the perplexity. Otherwise, it will just return the perplexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UozYDTgTPYBN"
   },
   "source": [
    "## Word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DIQhiqR2joDx"
   },
   "source": [
    "Start by running the cell below, which will load a large matrix. During loading, which will take some time, you can continue to read the explanation below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1PSMzMojSBb2",
    "outputId": "d65bfd2c-8eab-404d-cb8b-2a492e4c4ffa"
   },
   "outputs": [],
   "source": [
    "url_emb_matrix = 'http://homes.esat.kuleuven.be/~spchlab/H02A6/lab/session6/models/emb_matrix_ptb_256h.txt'\n",
    "emb_matrix = np.loadtxt(urllib.request.urlopen(url_emb_matrix))\n",
    "\n",
    "print('Size of the embedding matrix: {0}'.format(emb_matrix.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yUR-tGjo7x-d"
   },
   "source": [
    "Often the input words for a language model are represented as indices in a vocabulary, or **one-hot vectors** (where all values are 0 except the index of the word, which has value 1). This representation is a discrete representation, just like in n-gram language models. It has the disadvantage that relationships between words (e.g. the syntactic relationship between 'eat' and 'eating', or the semantic relationship between 'eat' and 'drink') cannot be inferred from the word representations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QNYBzUniRG1z"
   },
   "source": [
    "Neural language models however, do not use this representation as is but first map it to a continuous, lower-dimensional vector, also called **word embedding**. They do this by looking up the index of the word in a weight matrix $\\mathbf{E}$, which is often called the embedding matrix. By training the embedding matrix jointly with the rest of the language model, the resulting word embeddings will have some interesting properties: several syntactic and semantic relationships are encoded as vector offsets in the embedding space. A famous example is the vector offset for male - female, which is shown in the example below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M3DDEbxR7aka"
   },
   "source": [
    "![alt text](https://github.com/lverwimp/RNN_language_modeling/blob/master/kingqueen.png?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PRSLTNH9kC4m"
   },
   "source": [
    "The embedding matrix that we loaded above contains the embeddings of a large language model trained on Penn TreeBank. We will not train such a large model because the training time is too long (ca. 1h, depending on the hardware). In the next chapter, we will train smaller models on a subset of Penn TreeBank, which can be trained in a couple of minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZowKb-VyqZp2"
   },
   "source": [
    "We will now take a look at what the embeddings look like. Simply looking at their values does not tell us much:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DUt7sr47qk3H",
    "outputId": "5b604050-db1e-4127-8d38-c816f08e17c2"
   },
   "outputs": [],
   "source": [
    "print(\"This is what the embedding for 'man' looks like: {0}\".format(emb_matrix[item_to_id['man']]))\n",
    "print(\"This is what the embedding for 'woman' looks like: {0}\".format(emb_matrix[item_to_id['woman']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e0iRSNUmdJKs"
   },
   "source": [
    "\n",
    "\n",
    "*   What do you observe in the embeddings above? \n",
    "*   What could you do to make the embedding more interpretable?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "osPB09mvrJcy"
   },
   "outputs": [],
   "source": [
    "# if you want you can look at the embedding values of another word you're interesting in\n",
    "# simply change 'woman' in the print statement below\n",
    "# !!! note: there will be a KeyError if you try a word that is not in the vocabulary\n",
    "# print(\"This is what the embedding for 'woman' looks like: {0}\".format(emb_matrix[item_to_id['woman']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zueoWNr0rpbN"
   },
   "source": [
    "A common technique to inspect embeddings is **dimensionality reduction**, which reduces the high-dimensional vector (in our case 256) to a 2- or 3-dimensional vector which still captures the most important relationships. The simplest dimensionality reduction technique is principal component analysis (PCA). How PCA exactly works, is not important here, but we will use it to map our embeddings to a 2-dimensional space in the code below. We define a function that plots a subset of words in this 2-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D0g6ZWggDmqa"
   },
   "outputs": [],
   "source": [
    "# import the libraries that we need\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "    \n",
    "# perform principal component analysis\n",
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(emb_matrix)\n",
    "\n",
    "def plot_pca(list_words):\n",
    "  '''\n",
    "  Plot all words in 'list_words' on the 2-dimensional PCA space.\n",
    "  '''\n",
    "  if len(list_words) > 10:\n",
    "    raise IOError(\"Maximum 10 words can be plotted.\")\n",
    "    \n",
    "  for w in list_words:\n",
    "    if w not in item_to_id:\n",
    "      list_words.remove(w)\n",
    "      print('Ignoring {0} because it is not in the vocabulary'.format(w))\n",
    "\n",
    "  colors = ['navy','turquoise','darkorange','red','black','blue','yellow','green','purple','pink']\n",
    "\n",
    "  for color, target_name in zip(colors[:len(list_words)], list_words):\n",
    "      plt.scatter(principalComponents[item_to_id[target_name], 0], \n",
    "                  principalComponents[item_to_id[target_name], 1], \n",
    "                  color=color, \n",
    "                  label=target_name)\n",
    "  plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gef82K2asdbc"
   },
   "source": [
    "You can use the plot_pca function with a maximum of 10 words, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "CzdPrkgpoID4",
    "outputId": "6ab3f75f-d74d-47cd-b749-64159d788691"
   },
   "outputs": [],
   "source": [
    "plot_pca(['man', 'woman', 'king', 'queen','men','women','child','children','boy','girl'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vFUhVLlDtEVN"
   },
   "source": [
    "*   Which relationships do you observe in the plot above?\n",
    "*   What relationships did you expect? \n",
    "*   Are these relationships actually present in the plot? If not, why do you think they are not visible?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z3TUd6BPWQJc"
   },
   "source": [
    "Another way to inspect word embeddings is to look at words that are closest to a specific target word. Closeness in a vector space can be calculated by using for example cosine similarity. In the function below, we calculate for a given word the top 10 closest words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tKepamr6R-vX"
   },
   "outputs": [],
   "source": [
    "def find_closest_words(word):\n",
    "  if word not in item_to_id:\n",
    "    raise IOError('This item is not in the vocabulary')\n",
    "    \n",
    "  else:\n",
    "    id_w = item_to_id[word]\n",
    "    emb_w = emb_matrix[id_w]\n",
    "    # normalize the embedding vector unit length\n",
    "    norm_emb_w = emb_w / np.linalg.norm(emb_w)\n",
    "    \n",
    "    top_10 = {}\n",
    "    \n",
    "    # iterate over all words\n",
    "    for idx in range(emb_matrix.shape[0]):\n",
    "      # ignore the word itself\n",
    "      if idx != id_w:\n",
    "        \n",
    "        # normalize to unit length\n",
    "        norm_curr_w = emb_matrix[idx] / np.linalg.norm(emb_matrix[idx])\n",
    "        \n",
    "        # cosine similarity = dot product of normalized vectors\n",
    "        cos_sim = np.dot(norm_emb_w, norm_curr_w)\n",
    "        \n",
    "        # keep list of top 10 largest cos similarities\n",
    "        if len(top_10) >= 10:\n",
    "          for sim in top_10.keys():\n",
    "            if cos_sim > sim:\n",
    "              \n",
    "              del top_10[sim]\n",
    "              top_10[cos_sim] = id_to_item[idx]\n",
    "              break\n",
    "        \n",
    "        else:\n",
    "          top_10[cos_sim] = id_to_item[idx]\n",
    "          \n",
    "        \n",
    "    print('Words with largest cosine similarity w.r.t. {0}'.format(word))\n",
    "    # sort the top 10 \n",
    "    for sim in sorted(top_10, key=float, reverse=True):\n",
    "      print('{0} ({1})'.format(top_10[sim], sim))\n",
    "    print()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YnvF6mD2XG3d"
   },
   "source": [
    "The function above can be called as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nqy54srADglI",
    "outputId": "dece4e27-6481-4c2d-9c34-94c04b407e8c"
   },
   "outputs": [],
   "source": [
    "find_closest_words('man')\n",
    "find_closest_words('help')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a2IsL_fxXJ0f"
   },
   "source": [
    "Notice that we did not optimize our embedding space to contain any relationships - this is merely a by-product of training a neural language model. \n",
    "\n",
    "*   Which relationships do you see between the target word and its closest words?\n",
    "*   What kind of relationships do you see: syntactic and/or semantic?\n",
    "*   Do all closest words have clear relationships with the target word?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "auWYJWXdrlLU"
   },
   "source": [
    "## PERSONAL EXERCISE -- PREPARATION FOR THE EXAM\n",
    "\n",
    "This sections contains a personalized variant on the exercise(s) above.   **You need to bring the printout of this COLAB exercise to the exam**, where it will be used to ask you some further personalized questions.\n",
    "\n",
    "There are 2 code cells in this section:\n",
    "\n",
    "**[1]** This cell will generate your personal word list for this exercise.  Enter your KULeuven ID and execute the cell.  \n",
    " + it's not so relevant how this happens but don't modify the hidden parameters ! (you can play with them, but the exercise you bring to the exam should be based on the default settings)\n",
    " + first your *personal key* is generated; it is a nonsense word using your id as seed and fitting English letter 4gram probabilities.  So it may actually look like an English word, but most likely it isn't\n",
    " + then your *personal wordlist* is generated; this consists of the top-3 words from the 2500 most frequent words in the training database that are closest (DTW distance) to your *personal key*.\n",
    "  \n",
    "**[2]** This cell runs the find_closest_words() routine on your wordlist and hence returns words that are the nearest neighbours in embedding space for the words in your wordlist. \n",
    "For each of your personal words, give a motivation explanation which ones of the close neighbours in embedded space are plausible in your opionion.  \n",
    " \n",
    "**Bring the full output of this COLAB cell to the exam, including key, wordlist and nearest neighbours**.   \n",
    "\n",
    "**[3]** (optional) In this cell you can plot via PCA the relationships of the words in your wordlist.  It is rather unlikely that you will see much here. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aKmwYsPG_bVi",
    "outputId": "79a6f7fe-3e9d-4cad-add1-349d08adddc6"
   },
   "outputs": [],
   "source": [
    "import random, re\n",
    "kul_id = 's0123456' #@param {type:\"string\"}\n",
    "vocsize = 2500 \n",
    "selection = 3 \n",
    "length = 8 \n",
    "ngram_length = 4 \n",
    "# UTILITY: a random word generator made up of frequent letter ngrams\n",
    "def random_word(length=10,seed='u012345',text='abcdefghjk',ng=5):\n",
    "  chars = [text[i:i + ng] for i in range(0, len(text)-ng+1) if ' ' not in text[i:i+ng]]\n",
    "  char_counter = collections.Counter(chars)\n",
    "  ngrams= ([w for (w,n) in char_counter.most_common(100)])\n",
    "  random.seed(seed,version=2)\n",
    "  nwords = length//ng + 1\n",
    "  ngram_word = \"\".join(random.sample(ngrams,nwords))\n",
    "  return(ngram_word[0:length])\n",
    "######### generate your personal key  #####################\n",
    "text = train_file.decode(encoding='utf-8', errors='strict').lower().replace('\\n',' ').replace('<unk>',' ').replace('$',' ')\n",
    "my_key = random_word(length=length,seed=kul_id,text=text.replace(' ',''),ng=ngram_length)  \n",
    "#print(\"Generated Key: \", my_key)\n",
    "\n",
    "### dtw utility\n",
    "def stringdtw(str1,str2):\n",
    "  s1 = '#'+str1\n",
    "  s2 = '#'+str2\n",
    "  # initialize\n",
    "  Dist = [i for i in range(0,len(s2)) ] \n",
    "  Dist[0] = 0\n",
    "  for i in range(1,len(s1)):\n",
    "    Dist1 = Dist.copy()\n",
    "    Dist[0] = Dist1[0] + 1\n",
    "    for j in range(1,len(s2)):\n",
    "      Dist[j] =  min(\n",
    "          Dist1[j-1] + int(s1[i]!=s2[j]) ,\n",
    "          Dist1[j] + 1.2,\n",
    "          Dist[j-1] + 1.2)\n",
    "  return(Dist[len(s2)-1])\n",
    "#############################################################################\n",
    "\n",
    "######### generate your personal word list ################\n",
    "wlist = most_frequent_words[0:vocsize]\n",
    "dist = np.zeros(vocsize,dtype='float')\n",
    "for i in range(vocsize):\n",
    "  dist[i] = stringdtw(wlist[i],my_key)\n",
    "sorted_wordlist = [wlist[i] for i in np.argsort(dist) if len(wlist[i]) >= length-2]\n",
    "my_wordlist = sorted_wordlist[0:selection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7aUrKQSdAnc5",
    "outputId": "af102138-c1b9-4313-8ef2-bed301c98c8e"
   },
   "outputs": [],
   "source": [
    "######### find the closest words to your personal wordlist according to embeddings  ##############\n",
    "print(\"=\"*70)\n",
    "print(\"Your KULeuven ID:\", kul_id)\n",
    "print(\"Your Personal Key:\",my_key)\n",
    "print(\"Your Personal Wordlist: \", my_wordlist)\n",
    "print(\"=\"*70)\n",
    "for w in my_wordlist:\n",
    "  find_closest_words(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "IuHhX4wwsogJ",
    "outputId": "a2c3e215-8675-456c-ace4-bc41b75d719e"
   },
   "outputs": [],
   "source": [
    "# You can look for similarities in your word list, but the words in this list don't have much in common, \n",
    "# so you probably won't see much, but you can trye\n",
    "plot_pca(sorted_wordlist[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PpoaAoNOX6qR"
   },
   "source": [
    "Notice that not all words will have sensible nearest neighbours.  The main reason is that the words in your wordlist might have verry little in common\n",
    "(this was not part of the selection criterion).  \n",
    "\n",
    "More generally, the model is trained on a relatively small dataset. Not all words will have large enough frequency and the model is not optimized to encode these relationships.  Another anomaly due to the small training dataset is that quite unexpected neighbours pop up in the list. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9sYewPcLl3WG"
   },
   "source": [
    "## Testing networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nr8m1gNedwnp"
   },
   "source": [
    "Before going into the details of training a neural language model, we will first show how you can use a trained model. The output of the neural network is given to a **softmax** function, which converts the output values (also called logits) to values between 0 and 1. The sum of those values is 1, and thus the output of the softmax function can be treated as a probability distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DeTdStZphMnH"
   },
   "source": [
    "We can then find the probability of a specific word  following the current input word by looking up its probability in the output vector of the softmax function. To find the most probable word, we look for the maximum probability. In practice, we usually work with **log probabilities**, because if we are computing the probability of a sequence of words, the multiplication of all probabilities easily becomes very small. Converting the probabilities to the log domain and summing them instead of multiplying alleviates this problem. The log probability of a sentence is then sum of the log probabilities of every word in the sentence, given their context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQ0wAhRNmATx"
   },
   "source": [
    "Let's first train a network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dQYIjUpZipvD",
    "outputId": "8d07638a-9e38-47fb-fdfc-a8edbab6c6ee"
   },
   "outputs": [],
   "source": [
    "# first make sure that we start training the model from scratch, by removing the models/ directory\n",
    "!rm -rf baseline\n",
    "\n",
    "# train the model\n",
    "run_lm(name='baseline', train_ids=train_ids, valid_ids=valid_ids, test_ids=test_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WhOFC3M2kMnC"
   },
   "source": [
    "During training, not the log probabilities are printed but the **perplexity** of the model. Perplexity is commonly used to measure the quality of a language model, and corresponds to $$e^{\\frac{1}{N}~ln~P(x_1~\\ldots~x_N)}$$ You see that it is closely related to the log probability. The lower the perplexity, the better. Notice that in the example above the perplexities are quite high because we are training a small model on a small dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E1HZscovkEMg"
   },
   "source": [
    "To test the model we define a function that prints the log probability of a sentence. The sentence is converted to indices and then given to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TaZ-Wle_C4Pk"
   },
   "outputs": [],
   "source": [
    "def get_log_prob(test_sent):\n",
    "  \n",
    "  # convert words to indices\n",
    "  test_idx = []\n",
    "  for w in test_sent.split(' '):\n",
    "    if w not in item_to_id:\n",
    "      raise IOError(\"{0} is not part of the vocabulary\".format(w))\n",
    "    else:\n",
    "      test_idx.append(item_to_id[w])\n",
    "\n",
    "  # feed sentence to the model\n",
    "  run_lm(name='baseline',\n",
    "                test_ids=test_idx,\n",
    "                test_log_prob=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4sp8fOR8mPZ2"
   },
   "source": [
    "To get the log probability of a specific sentence, use the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XMoIfVfSZm1D",
    "outputId": "e22b7870-4f3a-4b2a-dd83-b52c05d9f61b"
   },
   "outputs": [],
   "source": [
    "get_log_prob(test_sent='this is a test <eos>')\n",
    "get_log_prob(test_sent='test a a a <eos>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cvta5ZrkeYlH"
   },
   "source": [
    "\n",
    "\n",
    "*   Which sentence has the largest probability? Is this what you expected?\n",
    "*   What would happen if you compare the probability of two sentences with an unequal number of words? Remember that the probability of a sentence = product of the probabilities of each word = sum of the log probabilities of each word.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dvXDxzgFmcQv"
   },
   "source": [
    "You can test your own sentences here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pdmBlzRwmi94",
    "outputId": "2152bee4-818c-4749-c40f-457caae7700d"
   },
   "outputs": [],
   "source": [
    "get_log_prob('good things happen to good people')\n",
    "get_log_prob('bad things happen to bad people')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IbEp9tdFKXA4"
   },
   "source": [
    "## Training networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a61RdtDCKhmp"
   },
   "source": [
    "Training neural networks requires a lot of hyperparameter tuning. The hyperparameters of a neural network are for example the type of cell, its size, the method that is used for updating its parameters (also called 'optimizer' ), the type and strength of regularization, ... . All these hyperparameters have to be chosen before the network can built, trained and tested, and they all have to some extent an influence on the  performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BVTDtTYYw77W"
   },
   "source": [
    "The default arguments for our network are the following:\n",
    "\n",
    "\n",
    "* cell: 'LSTM'\n",
    "* optimizer: 'Adam'\n",
    "* lr: 0.01\n",
    "* embedding_size: 64\n",
    "* hidden_size: 128\n",
    "* dropout_rate: 0.5\n",
    "\n",
    "We will explain each of these arguments in the following sections.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ZaHDz1Bsqnt"
   },
   "source": [
    "### Type of cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j3bN2Mvmo_8M"
   },
   "source": [
    "Recurrent neural networks are neural networks that take as input a combination of the standard input and the hidden state of the previous time step. The simplest form of recurrent neural network, often called **vanilla recurrent neural network**, looks like this (picture taken from the Chris Olah's [blog post](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "swms9ZXrq1X0"
   },
   "source": [
    "![alt text](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-SimpleRNN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DIPmg4vwq3l5"
   },
   "source": [
    "The green blocks in the picture represent the neural network. You see that the inputs to the neural network are the current input word $\\mathbf{x}_t$ (this is the word embedding as discussed before) and the state of the previous time step $\\mathbf{h}_{t-1}$. The network multiplies both inputs with weights and applies a non-linearity, in this case $\\tanh$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nd57sRKcsEKY"
   },
   "source": [
    "The network that we trained in the previous section is not a vanilla recurrent neural network, but a **long short-term memory (LSTM)** network, which is a more powerful variant. If you're interested in knowing how the LSTM works, the blog post mentioned above is a great introduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "42u843fb9qq2"
   },
   "source": [
    "For comparison, let's now train a simple RNN instead of an LSTM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KRUa0rDYc7SC",
    "outputId": "756b68ba-1d25-4cd9-cdb9-9e74961b464b"
   },
   "outputs": [],
   "source": [
    "run_lm(name='RNN', cell='RNN', train_ids=train_ids, valid_ids=valid_ids, test_ids=test_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wr4JKWqNs7c_"
   },
   "source": [
    "Notice that the simple RNN performs much worse than the LSTM (the perplexities are much higher).\n",
    "\n",
    "*   How does the simple RNN compare with the LSTM that you trained in the previous section? Are the perplexities lower?\n",
    "*   Look at the evolution of the train perplexities and compare it with the evolution of the validation perplexities. Does a lower train perplexity automatically correspond to a lower validation perplexity? If this is not the case, why do you think this happens?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zI30kaseLmVT"
   },
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "osTYguHZprBE"
   },
   "source": [
    "Another important hyperparameter for neural networks is the type of optimizer. Training a neural network implies that you give an input to the network, calculate the output and the difference with the expected output, which is equal to the **error** or **loss**. To update the parameters based on the error, the **gradient** of the loss with respect to the parameters is calculated. The gradient tells you in which direction you need to move to maximize the loss. In our case, we want to minimize the loss so we will move in the opposite direction (negative gradient). The optimizer then decides how this gradient is used to change the parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gpv1uOXhufG1"
   },
   "source": [
    "The simplest option is to subtract (a scaled version of) the gradient from the parameters. This optimizer is called **stochastic gradient descent**. In the experiments in the previous section, we used another, more complicated, optimizer called **Adam**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FWJaqfh7vz8R"
   },
   "source": [
    "Let's now train a network with stochastic gradient descent instead of Adam:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dWILisHi1xHp",
    "outputId": "d3c296e0-a73b-4ebe-e386-c15d1d0ba561"
   },
   "outputs": [],
   "source": [
    "run_lm(name='SGD', optimizer='SGD', train_ids=train_ids, valid_ids=valid_ids, test_ids=test_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_jljDenfM7r"
   },
   "source": [
    "\n",
    "\n",
    "*   Are the perplexities better than the baseline model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8wKt7L9aPLfs"
   },
   "source": [
    "### Learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EXDniSW54NfB"
   },
   "source": [
    "Judging from the perplexities above, it seems like the Adam optimizer is the best choice for training our network. However, the interplay between the different hyperparameters of a neural network is complicated, and it is very well possible that a specific optimizer needs a different learning rate. \n",
    "Let's try SGD with a learning rate of 1 instead of the default 0.01:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pPl3KiSs5kQk",
    "outputId": "5388c931-fd3d-4394-e10a-209779495bb7"
   },
   "outputs": [],
   "source": [
    "run_lm(name='SGD_1', optimizer='SGD', lr=1.0, train_ids=train_ids, valid_ids=valid_ids, test_ids=test_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WcHT9i43fdxe"
   },
   "source": [
    "\n",
    "\n",
    "*   Does using a larger learning rate improve results for SGD?\n",
    "*   Is this result better than for the model optimized with Adam?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gcNjIbzgDUuk"
   },
   "source": [
    "Maybe using a larger learning rate helps in general? Let's try the same learning rate in combination with Adam:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lSH6WKIdxlAE",
    "outputId": "94bfbd7b-6169-4c3b-ec96-97fa21338bef"
   },
   "outputs": [],
   "source": [
    "run_lm(name='Adam_1', lr=1.0, train_ids=train_ids, valid_ids=valid_ids, test_ids=test_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B4rCiwgNftoq"
   },
   "source": [
    "\n",
    "\n",
    "*   Does using a larger learning rate for Adam work better?\n",
    "*   What can you conclude about the interplay between optimizer and learning rate, based on these experiments?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-XqENWWbyCdW"
   },
   "source": [
    "Let's now reduce the learning rate for Adam, from 0.01 to 0.001:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UyUTwGvF7Iyy",
    "outputId": "0335f9d7-b303-4ace-c8d2-91bc59c82132"
   },
   "outputs": [],
   "source": [
    "run_lm(name='Adam_0.001', lr=0.001, train_ids=train_ids, valid_ids=valid_ids, test_ids=test_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nNkyWjQNCjja"
   },
   "source": [
    "\n",
    "\n",
    "*   Is Adam with a learning rate of 0.001 better than with a learning rate of 0.01? \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MGAY_hhfNLj5"
   },
   "source": [
    "### Size of the embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jZ87PmaPJlib"
   },
   "source": [
    "Let's now take a look at the influence of the size of the LSTM on its performance. By default, we train a model with embeddings of size 64 and a hidden layer of size 128. Let's see what happens if we reduce the size of the embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q2d_Kx8zJGk-",
    "outputId": "70c41c9b-b4bc-4750-86e4-39b5f74e4d98"
   },
   "outputs": [],
   "source": [
    "run_lm(name='emb16', embedding_size=16, train_ids=train_ids, valid_ids=valid_ids, test_ids=test_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jJ98c5mGgLJR"
   },
   "source": [
    "\n",
    "\n",
    "*   Is using a smaller embedding better or worse? Is there a large difference?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHf3oNWgzEyG"
   },
   "source": [
    "What about a larger embedding size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3D6D-G01zIN5",
    "outputId": "0816c71c-5c82-45ba-ccfc-b5f124b71d66"
   },
   "outputs": [],
   "source": [
    "run_lm(name='emb128', embedding_size=128, train_ids=train_ids, valid_ids=valid_ids, test_ids=test_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WSpgrmJa7-8L"
   },
   "source": [
    "*   Is using a larger embedding better or worse? Is there a large difference?\n",
    "*   The size of the vocabulary of our language model is 10.000, which is quite small. If we trained a language model with a larger vocabulary size (e.g. 100.000), what do you expect with respect to the relative differences between embedding sizes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ranM0ERNIta"
   },
   "source": [
    "### Size of the hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n9huic9_8Het"
   },
   "source": [
    "We can also change the size of the hidden layer/the number of neurons in the network. In principle you also choose to increase the number of layers, but this is mainly useful for large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PF81eBTK8b5B"
   },
   "source": [
    "Let's first test a smaller hidden layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vbkbvBaWLVaj",
    "outputId": "aae0dbe4-2560-407f-a17f-269b13611bc6"
   },
   "outputs": [],
   "source": [
    "run_lm(name='hidden64', hidden_size=64, train_ids=train_ids, valid_ids=valid_ids, test_ids=test_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YjSUxuNm8jVA"
   },
   "source": [
    "*   Is using a smaller hidden layer better or worse? Is there a large difference?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FOhM057eh8iH"
   },
   "source": [
    "Let's now train a model with a larger hidden layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1XrNmCN88ewu",
    "outputId": "486e55f7-84c9-4ef6-8fa9-2425d8b2bae0"
   },
   "outputs": [],
   "source": [
    "run_lm(name='hidden256', hidden_size=256, train_ids=train_ids, valid_ids=valid_ids, test_ids=test_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CxoqYlUv9FUE"
   },
   "source": [
    "*   Is using a smaller hidden layer better or worse? Is there a large difference?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YoxB_IURiCE_"
   },
   "source": [
    "Let's now train a baseline language model (with a hidden size of 128) on the full Penn TreeBank data set by using {train/valid/test}_ids_large:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RrmToOiiLlj0",
    "outputId": "f6285c17-0fa4-4320-bec7-cff1aca68c71"
   },
   "outputs": [],
   "source": [
    "run_lm(name='large', train_ids=train_ids_large, valid_ids=valid_ids_large, test_ids=test_ids_large)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dqMsm4Uyils8"
   },
   "source": [
    "\n",
    "\n",
    "*   Can you directly compare the perplexity of this model with the perplexity of the model trained on the small dataset? Remember how perplexity is calculated. Are the validation and test sets the same for the small and large dataset?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kZdunTOcMEAL"
   },
   "source": [
    "Let's now train a model with a larger hidden size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UqZOMze8AMIJ",
    "outputId": "bb7ea076-c67a-40f4-96b7-2c226cc9071c"
   },
   "outputs": [],
   "source": [
    "run_lm(name='large_hidden256', hidden_size=256, \n",
    "              train_ids=train_ids_large, valid_ids=valid_ids_large, test_ids=test_ids_large)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1uyXYwyiNgfO"
   },
   "source": [
    "\n",
    "\n",
    "*   Do you see improvement by using a larger hidden size?\n",
    "*   Is this improvement relatively larger or smaller than the improvement you observed for the small data set?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v1Ya3bs084Rb"
   },
   "source": [
    "### Try it yourself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KnlnlYWSMOA0"
   },
   "source": [
    "Now try some different values for the hyperparameters that we discussed (cell, optimizer, lr, embedding_size, hidden_size):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K_uSCdPgMNXb"
   },
   "outputs": [],
   "source": [
    "# run_lm(name='my_own_model', cell='LSTM', optimizer='Adam', lr='0.01', embedding_size='64', hidden_size='128', \n",
    "#        train_ids=train_ids, valid_ids=valid_ids, test_ids=test_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hJ4EmqovMa7x"
   },
   "source": [
    "The hyperparameters that we discussed here are only a small subset, there exist many more that can have an influence on the performance of the neural network. Another important class of hyperparameters is related to **regularization**. Since neural networks contain many parameters, they can easily start **overfitting**, which means that the network starts memorizing the training set and cannot generalize well to new data sets anymore. Two important methods are dropout (setting a proportion of the neurons to 0 during training) and early stopping (stop training if the performance on the validation set decreases), but there exist many more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UuMMhuCukTDy"
   },
   "source": [
    "\n",
    "\n",
    "*   Have you seen examples of overfitting in the networks that we trained in this section? \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "7hmCKPx4KEcG",
    "FIhqMJ2CKMxP"
   ],
   "include_colab_link": true,
   "name": "rnn_lm.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
